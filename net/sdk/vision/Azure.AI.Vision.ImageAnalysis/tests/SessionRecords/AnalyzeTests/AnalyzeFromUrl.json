{
  "Entries": [
    {
      "RequestUri": "https://ResourceName.cognitiveservices.azure.com/computervision/imageanalysis:analyze?features=tags%2Ccaption%2CdenseCaptions%2Cobjects%2Cread%2CsmartCrops%2Cpeople\u0026api-version=2023-04-01-preview\u0026smartcrops-aspect-ratios=0.9%2C1.33",
      "RequestMethod": "POST",
      "RequestHeaders": {
        "Accept": "application/json",
        "Content-Length": "62",
        "Content-Type": "application/json",
        "Ocp-Apim-Subscription-Key": "***********",
        "traceparent": "00-06ddaade2055e87665cde7f2718de932-b1c39b449c54edbd-00",
        "User-Agent": "azsdk-net-AI.Vision.ImageAnalysis/1.0.0-alpha.20231129.1 (.NET 7.0.14; Microsoft Windows 10.0.22621)",
        "x-ms-client-request-id": "0e9be3f6a4ff185268ada820e8856785",
        "x-ms-return-client-request-id": "true"
      },
      "RequestBody": {
        "url": "https://aka.ms/azai/vision/image-analysis-sample.jpg"
      },
      "StatusCode": 200,
      "ResponseHeaders": {
        "api-supported-versions": "4.0-pre1,2022-07-31-preview,2022-10-12-preview,2023-02-01-preview,2023-04-01-preview,2023-06-01-preview,2023-07-01-preview,2023-10-01",
        "apim-request-id": "3c023cc7-7201-4a9f-94fa-2d9313e5b7fb",
        "Content-Length": "4506",
        "Content-Type": "application/json; charset=utf-8",
        "CSP-Billing-Usage": "CognitiveServices.ComputerVision.Transaction=1",
        "Date": "Wed, 29 Nov 2023 19:05:33 GMT",
        "ms-vision-response-time-input-processed": "939",
        "ms-vision-response-time-input-received": "1340",
        "request-id": "3c023cc7-7201-4a9f-94fa-2d9313e5b7fb",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
        "X-Content-Type-Options": "nosniff",
        "x-envoy-upstream-service-time": "2284",
        "x-ms-region": "East US"
      },
      "ResponseBody": {
        "captionResult": {
          "text": "a woman wearing a mask sitting at a table with a laptop",
          "confidence": 0.8502674698829651
        },
        "objectsResult": {
          "values": [
            {
              "boundingBox": {
                "x": 603,
                "y": 225,
                "w": 152,
                "h": 224
              },
              "tags": [
                {
                  "name": "chair",
                  "confidence": 0.618
                }
              ]
            },
            {
              "boundingBox": {
                "x": 399,
                "y": 244,
                "w": 249,
                "h": 325
              },
              "tags": [
                {
                  "name": "person",
                  "confidence": 0.881
                }
              ]
            },
            {
              "boundingBox": {
                "x": 295,
                "y": 387,
                "w": 211,
                "h": 102
              },
              "tags": [
                {
                  "name": "Laptop",
                  "confidence": 0.767
                }
              ]
            },
            {
              "boundingBox": {
                "x": 441,
                "y": 436,
                "w": 256,
                "h": 136
              },
              "tags": [
                {
                  "name": "chair",
                  "confidence": 0.581
                }
              ]
            },
            {
              "boundingBox": {
                "x": 123,
                "y": 437,
                "w": 460,
                "h": 125
              },
              "tags": [
                {
                  "name": "dining table",
                  "confidence": 0.606
                }
              ]
            }
          ]
        },
        "readResult": {
          "stringIndexType": "TextElements",
          "content": "Sample text\nHand writing\n123 456",
          "pages": [
            {
              "height": 576.0,
              "width": 864.0,
              "angle": 0.0,
              "pageNumber": 1,
              "words": [
                {
                  "content": "Sample",
                  "boundingBox": [
                    722.0,
                    502.0,
                    785.0,
                    502.0,
                    785.0,
                    520.0,
                    722.0,
                    520.0
                  ],
                  "confidence": 0.994,
                  "span": {
                    "offset": 0,
                    "length": 6
                  }
                },
                {
                  "content": "text",
                  "boundingBox": [
                    798.0,
                    502.0,
                    841.0,
                    501.0,
                    841.0,
                    519.0,
                    798.0,
                    519.0
                  ],
                  "confidence": 0.989,
                  "span": {
                    "offset": 7,
                    "length": 4
                  }
                },
                {
                  "content": "Hand",
                  "boundingBox": [
                    721.0,
                    526.0,
                    759.0,
                    526.0,
                    758.0,
                    543.0,
                    720.0,
                    543.0
                  ],
                  "confidence": 0.982,
                  "span": {
                    "offset": 12,
                    "length": 4
                  }
                },
                {
                  "content": "writing",
                  "boundingBox": [
                    764.0,
                    526.0,
                    819.0,
                    527.0,
                    818.0,
                    544.0,
                    763.0,
                    543.0
                  ],
                  "confidence": 0.994,
                  "span": {
                    "offset": 17,
                    "length": 7
                  }
                },
                {
                  "content": "123",
                  "boundingBox": [
                    723.0,
                    548.0,
                    749.0,
                    548.0,
                    749.0,
                    565.0,
                    722.0,
                    565.0
                  ],
                  "confidence": 0.958,
                  "span": {
                    "offset": 25,
                    "length": 3
                  }
                },
                {
                  "content": "456",
                  "boundingBox": [
                    758.0,
                    549.0,
                    787.0,
                    549.0,
                    787.0,
                    565.0,
                    758.0,
                    565.0
                  ],
                  "confidence": 0.998,
                  "span": {
                    "offset": 29,
                    "length": 3
                  }
                }
              ],
              "spans": [
                {
                  "offset": 0,
                  "length": 32
                }
              ],
              "lines": [
                {
                  "content": "Sample text",
                  "boundingBox": [
                    721.0,
                    502.0,
                    843.0,
                    500.0,
                    843.0,
                    518.0,
                    721.0,
                    519.0
                  ],
                  "spans": [
                    {
                      "offset": 0,
                      "length": 11
                    }
                  ]
                },
                {
                  "content": "Hand writing",
                  "boundingBox": [
                    720.0,
                    525.0,
                    819.0,
                    526.0,
                    819.0,
                    543.0,
                    720.0,
                    542.0
                  ],
                  "spans": [
                    {
                      "offset": 12,
                      "length": 12
                    }
                  ]
                },
                {
                  "content": "123 456",
                  "boundingBox": [
                    722.0,
                    548.0,
                    790.0,
                    548.0,
                    790.0,
                    564.0,
                    722.0,
                    564.0
                  ],
                  "spans": [
                    {
                      "offset": 25,
                      "length": 7
                    }
                  ]
                }
              ]
            }
          ],
          "styles": [],
          "modelVersion": "2022-04-30"
        },
        "denseCaptionsResult": {
          "values": [
            {
              "text": "a woman wearing a mask sitting at a table with a laptop",
              "confidence": 0.8502674698829651,
              "boundingBox": {
                "x": 0,
                "y": 0,
                "w": 864,
                "h": 576
              }
            },
            {
              "text": "a person using a laptop",
              "confidence": 0.7724273204803467,
              "boundingBox": {
                "x": 293,
                "y": 383,
                "w": 195,
                "h": 100
              }
            },
            {
              "text": "a woman wearing a face mask",
              "confidence": 0.8215275406837463,
              "boundingBox": {
                "x": 383,
                "y": 233,
                "w": 275,
                "h": 336
              }
            },
            {
              "text": "a close-up of a green chair",
              "confidence": 0.8763102889060974,
              "boundingBox": {
                "x": 616,
                "y": 211,
                "w": 164,
                "h": 249
              }
            },
            {
              "text": "a person wearing a colorful cloth face mask",
              "confidence": 0.7086368799209595,
              "boundingBox": {
                "x": 473,
                "y": 294,
                "w": 68,
                "h": 56
              }
            },
            {
              "text": "a person using a laptop",
              "confidence": 0.7642353177070618,
              "boundingBox": {
                "x": 288,
                "y": 211,
                "w": 151,
                "h": 244
              }
            },
            {
              "text": "a woman wearing a colorful fabric face mask",
              "confidence": 0.7735569477081299,
              "boundingBox": {
                "x": 433,
                "y": 240,
                "w": 180,
                "h": 236
              }
            },
            {
              "text": "a close-up of a laptop on a table",
              "confidence": 0.8536830544471741,
              "boundingBox": {
                "x": 115,
                "y": 443,
                "w": 476,
                "h": 125
              }
            },
            {
              "text": "a woman wearing a mask and using a laptop",
              "confidence": 0.7810136675834656,
              "boundingBox": {
                "x": 0,
                "y": 0,
                "w": 774,
                "h": 432
              }
            },
            {
              "text": "a close up of a text",
              "confidence": 0.6402827501296997,
              "boundingBox": {
                "x": 714,
                "y": 493,
                "w": 130,
                "h": 80
              }
            }
          ]
        },
        "modelVersion": "2023-02-01-preview",
        "metadata": {
          "width": 864,
          "height": 576
        },
        "tagsResult": {
          "values": [
            {
              "name": "furniture",
              "confidence": 0.9874444007873535
            },
            {
              "name": "clothing",
              "confidence": 0.9792501926422119
            },
            {
              "name": "person",
              "confidence": 0.9427268505096436
            },
            {
              "name": "houseplant",
              "confidence": 0.9400021433830261
            },
            {
              "name": "desk",
              "confidence": 0.9182863235473633
            },
            {
              "name": "indoor",
              "confidence": 0.8963587880134583
            },
            {
              "name": "laptop",
              "confidence": 0.8781791925430298
            },
            {
              "name": "computer",
              "confidence": 0.8481509685516357
            },
            {
              "name": "sitting",
              "confidence": 0.8134787678718567
            },
            {
              "name": "wall",
              "confidence": 0.7511621117591858
            },
            {
              "name": "woman",
              "confidence": 0.7410725951194763
            },
            {
              "name": "table",
              "confidence": 0.6811204552650452
            },
            {
              "name": "plant",
              "confidence": 0.6445217132568359
            },
            {
              "name": "using",
              "confidence": 0.5358914136886597
            }
          ]
        },
        "smartCropsResult": {
          "values": [
            {
              "aspectRatio": 0.9,
              "boundingBox": {
                "x": 238,
                "y": 0,
                "w": 511,
                "h": 568
              }
            },
            {
              "aspectRatio": 1.33,
              "boundingBox": {
                "x": 54,
                "y": 0,
                "w": 760,
                "h": 571
              }
            }
          ]
        },
        "peopleResult": {
          "values": [
            {
              "boundingBox": {
                "x": 395,
                "y": 241,
                "w": 261,
                "h": 333
              },
              "confidence": 0.9602553248405457
            },
            {
              "boundingBox": {
                "x": 831,
                "y": 246,
                "w": 31,
                "h": 255
              },
              "confidence": 0.0016505217645317316
            }
          ]
        }
      }
    },
    {
      "RequestUri": "https://ResourceName.cognitiveservices.azure.com/computervision/imageanalysis:analyze?features=caption%2Cread\u0026api-version=2023-04-01-preview\u0026smartcrops-aspect-ratios=0.9%2C1.33",
      "RequestMethod": "POST",
      "RequestHeaders": {
        "Accept": "application/json",
        "Content-Length": "62",
        "Content-Type": "application/json",
        "Ocp-Apim-Subscription-Key": "***********",
        "traceparent": "00-f030ecfdc370fc295494c912cd01bda4-d13787a11ac701d7-00",
        "User-Agent": "azsdk-net-AI.Vision.ImageAnalysis/1.0.0-alpha.20231129.1 (.NET 7.0.14; Microsoft Windows 10.0.22621)",
        "x-ms-client-request-id": "da48ab719ec6289f2617d201b0d22940",
        "x-ms-return-client-request-id": "true"
      },
      "RequestBody": {
        "url": "https://aka.ms/azai/vision/image-analysis-sample.jpg"
      },
      "StatusCode": 200,
      "ResponseHeaders": {
        "api-supported-versions": "4.0-pre1,2022-07-31-preview,2022-10-12-preview,2023-02-01-preview,2023-04-01-preview,2023-06-01-preview,2023-07-01-preview,2023-10-01",
        "apim-request-id": "a154c9b5-240b-45f2-abd0-e5e0c504dae2",
        "Content-Length": "1626",
        "Content-Type": "application/json; charset=utf-8",
        "CSP-Billing-Usage": "CognitiveServices.ComputerVision.Transaction=1",
        "Date": "Wed, 29 Nov 2023 19:05:35 GMT",
        "ms-vision-response-time-input-processed": "244",
        "ms-vision-response-time-input-received": "1156",
        "request-id": "a154c9b5-240b-45f2-abd0-e5e0c504dae2",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
        "X-Content-Type-Options": "nosniff",
        "x-envoy-upstream-service-time": "1403",
        "x-ms-region": "East US"
      },
      "ResponseBody": {
        "captionResult": {
          "text": "a woman wearing a mask sitting at a table with a laptop",
          "confidence": 0.8498482704162598
        },
        "readResult": {
          "stringIndexType": "TextElements",
          "content": "Sample text\nHand writing\n123 456",
          "pages": [
            {
              "height": 576.0,
              "width": 864.0,
              "angle": 0.0,
              "pageNumber": 1,
              "words": [
                {
                  "content": "Sample",
                  "boundingBox": [
                    722.0,
                    502.0,
                    785.0,
                    502.0,
                    785.0,
                    520.0,
                    722.0,
                    520.0
                  ],
                  "confidence": 0.994,
                  "span": {
                    "offset": 0,
                    "length": 6
                  }
                },
                {
                  "content": "text",
                  "boundingBox": [
                    798.0,
                    502.0,
                    841.0,
                    501.0,
                    841.0,
                    519.0,
                    798.0,
                    519.0
                  ],
                  "confidence": 0.989,
                  "span": {
                    "offset": 7,
                    "length": 4
                  }
                },
                {
                  "content": "Hand",
                  "boundingBox": [
                    721.0,
                    526.0,
                    759.0,
                    526.0,
                    758.0,
                    543.0,
                    720.0,
                    543.0
                  ],
                  "confidence": 0.982,
                  "span": {
                    "offset": 12,
                    "length": 4
                  }
                },
                {
                  "content": "writing",
                  "boundingBox": [
                    764.0,
                    526.0,
                    819.0,
                    527.0,
                    818.0,
                    544.0,
                    763.0,
                    543.0
                  ],
                  "confidence": 0.994,
                  "span": {
                    "offset": 17,
                    "length": 7
                  }
                },
                {
                  "content": "123",
                  "boundingBox": [
                    723.0,
                    548.0,
                    749.0,
                    548.0,
                    749.0,
                    565.0,
                    722.0,
                    565.0
                  ],
                  "confidence": 0.958,
                  "span": {
                    "offset": 25,
                    "length": 3
                  }
                },
                {
                  "content": "456",
                  "boundingBox": [
                    758.0,
                    549.0,
                    787.0,
                    549.0,
                    787.0,
                    565.0,
                    758.0,
                    565.0
                  ],
                  "confidence": 0.998,
                  "span": {
                    "offset": 29,
                    "length": 3
                  }
                }
              ],
              "spans": [
                {
                  "offset": 0,
                  "length": 32
                }
              ],
              "lines": [
                {
                  "content": "Sample text",
                  "boundingBox": [
                    721.0,
                    502.0,
                    843.0,
                    500.0,
                    843.0,
                    518.0,
                    721.0,
                    519.0
                  ],
                  "spans": [
                    {
                      "offset": 0,
                      "length": 11
                    }
                  ]
                },
                {
                  "content": "Hand writing",
                  "boundingBox": [
                    720.0,
                    525.0,
                    819.0,
                    526.0,
                    819.0,
                    543.0,
                    720.0,
                    542.0
                  ],
                  "spans": [
                    {
                      "offset": 12,
                      "length": 12
                    }
                  ]
                },
                {
                  "content": "123 456",
                  "boundingBox": [
                    722.0,
                    548.0,
                    790.0,
                    548.0,
                    790.0,
                    564.0,
                    722.0,
                    564.0
                  ],
                  "spans": [
                    {
                      "offset": 25,
                      "length": 7
                    }
                  ]
                }
              ]
            }
          ],
          "styles": [],
          "modelVersion": "2022-04-30"
        },
        "modelVersion": "2023-02-01-preview",
        "metadata": {
          "width": 864,
          "height": 576
        }
      }
    }
  ],
  "Variables": {
    "RandomSeed": "1865314177",
    "VISION_ENDPOINT": "https://cv-sdktest-eastus.cognitiveservices.azure.com/"
  }
}
