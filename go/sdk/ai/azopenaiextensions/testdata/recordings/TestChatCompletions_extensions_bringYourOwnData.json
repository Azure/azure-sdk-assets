{
  "Entries": [
    {
      "RequestUri": "https://Sanitized.openai.azure.com/openai/deployments/gpt-4-0613/chat/completions?api-version=2025-03-01-preview",
      "RequestMethod": "POST",
      "RequestHeaders": {
        "Accept": "application/json",
        "Accept-Encoding": "gzip",
        "Authorization": "Sanitized",
        "Content-Length": "329",
        "Content-Type": "application/json",
        "User-Agent": "azsdk-go-azopenai-extensions/v.0.1.0 (go1.23.6; linux) OpenAI/Go 0.1.0-beta.2",
        "X-Stainless-Arch": "x64",
        "X-Stainless-Lang": "go",
        "X-Stainless-Os": "Linux",
        "X-Stainless-Package-Version": "0.1.0-beta.2",
        "X-Stainless-Retry-Count": "0",
        "X-Stainless-Runtime": "go",
        "X-Stainless-Runtime-Version": "go1.23.6"
      },
      "RequestBody": {
        "messages": [
          {
            "content": "What does the OpenAI package do?",
            "role": "user"
          }
        ],
        "model": "gpt-4-0613",
        "max_tokens": 512,
        "data_sources": [
          {
            "parameters": {
              "authentication": {
                "type": "system_assigned_managed_identity"
              },
              "endpoint": "https://openainewsdktestsearch.search.windows.net",
              "index_name": "openai-readmes-index"
            },
            "type": "azure_search"
          }
        ]
      },
      "StatusCode": 200,
      "ResponseHeaders": {
        "apim-request-id": "1ee3e80b-1f20-4bbc-b4ca-f4454d6aab2a",
        "Connection": "keep-alive",
        "Content-Length": "15302",
        "Content-Type": "application/json; charset=UTF-8",
        "Date": "Fri, 28 Mar 2025 17:09:24 GMT",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
        "X-Content-Type-Options": "nosniff",
        "x-envoy-upstream-service-time": "9260",
        "x-ms-client-request-id": "Sanitized",
        "x-ms-region": "East US",
        "x-ratelimit-limit-requests": "10",
        "x-ratelimit-limit-tokens": "10000",
        "x-ratelimit-remaining-requests": "9",
        "x-ratelimit-remaining-tokens": "9488"
      },
      "ResponseBody": {
        "id": "Sanitized",
        "model": "gpt-4",
        "created": 1743181764,
        "object": "extensions.chat.completion",
        "choices": [
          {
            "index": 0,
            "finish_reason": "stop",
            "message": {
              "role": "assistant",
              "content": "The OpenAI package provides convenient access to the OpenAI REST API from various programming languages. It allows developers to interact with OpenAI's services, such as generating text completions, chat completions, text embeddings, image generation, audio transcription, and more.\n\nIn the .NET library, for instance, you can use the `OpenAIClient` class to interact with the API, and it provides methods for using the async API and handling chat completions with streaming [doc1]. \n\nThe Python library provides similar functionalities, with additional features for handling raw response data, streaming responses, and differentiating between `None` and `null` values in API responses [doc2][doc4]. \n\nThe Java library from Azure OpenAI provides methods for text completions, streaming text completions, chat completions, streaming chat completions, text embeddings, image generation, audio transcription, and audio translation [doc3].\n\nThe Node.js library supports TypeScript and various runtime environments like Node.js, Deno, Bun, Cloudflare Workers, Vercel Edge Runtime, Jest, and Nitro. However, it does not support React Native at this time [doc5].\n\nIn all cases, to use these libraries, you need an API key from OpenAI, which you can obtain by creating an account on the OpenAI platform [doc1].",
              "end_turn": true,
              "context": {
                "citations": [
                  {
                    "content": "# OpenAI .NET API library\n\n[![NuGet version](https://img.shields.io/nuget/vpre/openai.svg)](https://www.nuget.org/packages/OpenAI/absoluteLatest)\n\nThe OpenAI .NET library provides convenient access to the OpenAI REST API from .NET applications. \n\nIt is generated from our [OpenAPI specification](https://github.com/openai/openai-openapi) in collaboration with Microsoft.\n\n## Table of Contents\n\n- [Getting started](#getting-started)\n  - [Prerequisites](#prerequisites)\n  - [Install the NuGet package](#install-the-nuget-package)\n- [Using the client library](#using-the-client-library)\n  - [Namespace organization](#namespace-organization)\n  - [Using the async API](#using-the-async-api)\n  - [Using the `OpenAIClient` class](#using-the-openaiclient-class)\n- [How to use chat completions with streaming](#how-to-use-chat-completions-with-streaming)\n- [How to use chat completions with tools and function calling](#how-to-use-chat-completions-with-tools-and-function-calling)\n- [How to generate text embeddings](#how-to-generate-text-embeddings)\n- [How to generate images](#how-to-generate-images)\n- [How to transcribe audio](#how-to-transcribe-audio)\n- [How to use assistants with retrieval augmented generation (RAG)](#how-to-use-assistants-with-retrieval-augmented-generation-rag)\n- [How to use streaming and GPT-4o vision with assistants](#how-to-use-streaming-and-gpt-4o-vision-with-assistants)\n- [How to work with Azure OpenAI](#how-to-work-with-azure-openai)\n- [Advanced scenarios](#advanced-scenarios)\n  - [Using protocol methods](#using-protocol-methods)\n  - [Automatically retrying errors](#automatically-retrying-errors)\n\n## Getting started\n\n### Prerequisites\n\nTo call the OpenAI REST API, you will need an API key. To obtain one, first [create a new OpenAI account](https://platform.openai.com/signup) or [log in](https://platform.openai.com/login). Next, navigate to the [API key page](https://platform.openai.com/account/api-keys) and select \"Create new secret key\", optionally naming the key. Make sure to save your API key somewhere safe and do not share it with anyone.\n\n### Install the NuGet package\n\nAdd the client library to your .NET project with [NuGet](https://www.nuget.org/) using your IDE or the dotnet CLI:\n\n```cli\ndotnet add package OpenAI --prerelease\n```\n\nNote that the code examples included below were written using [.NET 8](https://dotnet.microsoft.com/download/dotnet/8.0). The OpenAI .NET library is compatible with all .NET Standard 2.0 applications but some code examples in this document may depend on newer language features.\n\n## Using the client library\n\nThe full API of this library can be found in the [api.md](https://github.com/openai/openai-dotnet/blob/main/api/api.md) file, and there are many [code examples](https://github.com/openai/openai-dotnet/tree/main/examples) to help. For instance, the following snippet illustrates the basic use of the chat completions API:\n\n```csharp\nusing OpenAI.Chat;\n\nChatClient client = new(model: \"gpt-4o\", Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\"));\n\nChatCompletion completion = client.CompleteChat(\"Say 'this is a test.'\");\n\nConsole.WriteLine($\"[ASSISTANT]: {completion}\");",
                    "title": "OpenAI .NET API library",
                    "url": "https://openainewsdkteststorage.blob.core.windows.net/fileupload-openai-readmes-index/dotnet.md",
                    "filepath": "dotnet.md",
                    "chunk_id": "2"
                  },
                  {
                    "content": "### How to tell whether `None` means `null` or missing\n\nIn an API response, a field may be explicitly `null`, or missing entirely; in either case, its value is `None` in this library. You can differentiate the two cases with `.model_fields_set`:\n\n```py\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n  print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n  print('Got json like {\"my_field\": null}.')\n```\n\n### Accessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing `.with_raw_response.` to any HTTP method call, e.g.,\n\n```py\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n  messages=[{\n  \"role\": \"user\",\n  \"content\": \"Say this is a test\",\n  }],\n  model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n```\n\nThese methods return an [`LegacyAPIResponse`](https://github.com/openai/openai-python/tree/main/src/openai/_legacy_response.py) object. This is a legacy class as we're changing it slightly in the next major version.\n\nFor the sync client this will mostly be the same with the exception\nof `content` &amp; `text` will be methods instead of properties. In the\nasync client, all methods will be async.\n\nA migration script will be provided &amp; the migration in general should\nbe smooth.\n\n#### `.with_streaming_response`\n\nThe above interface eagerly reads the full response body when you make the request, which may not always be what you want.\n\nTo stream the response body, use `.with_streaming_response` instead, which requires a context manager and only reads the response body once you call `.read()`, `.text()`, `.json()`, `.iter_bytes()`, `.iter_text()`, `.iter_lines()` or `.parse()`. In the async client, these are async methods.\n\nAs such, `.with_streaming_response` methods return a different [`APIResponse`](https://github.com/openai/openai-python/tree/main/src/openai/_response.py) object, and the async client returns an [`AsyncAPIResponse`](https://github.com/openai/openai-python/tree/main/src/openai/_response.py) object.\n\n```python\nwith client.chat.completions.with_streaming_response.create(\n  messages=[\n  {\n  \"role\": \"user\",\n  \"content\": \"Say this is a test\",\n  }\n  ],\n  model=\"gpt-3.5-turbo\",\n) as response:\n  print(response.headers.get(\"X-My-Header\"))\n\n  for line in response.iter_lines():\n  print(line)",
                    "title": "OpenAI Python API library",
                    "url": "https://openainewsdkteststorage.blob.core.windows.net/fileupload-openai-readmes-index/python.md",
                    "filepath": "python.md",
                    "chunk_id": "0"
                  },
                  {
                    "content": "## Key concepts\n\n## Examples\nThe following sections provide several code snippets covering some of the most common OpenAI service tasks, including:\n\n* [Text completions sample](#text-completions \"Text completions\")\n* [Streaming text completions sample](#streaming-text-completions \"Streaming text completions\")\n* [Chat completions sample](#chat-completions \"Chat completions\")\n* [Streaming chat completions sample](#streaming-chat-completions \"Streaming chat completions\")\n* [Embeddings sample](#text-embeddings \"Text Embeddings\")\n* [Image Generation sample](#image-generation \"Image Generation\")\n* [Audio Transcription sample](#audio-transcription \"Audio Transcription\")\n* [Audio Translation sample](#audio-translation \"Audio Translation\")\n\n### Legacy completions\n\nIt is generally preferable to use Chat Completions instead. However, Completions are still supported:\n\n``` java readme-sample-getCompletions\nList prompt = new ArrayList&lt;&gt;();\nprompt.add(\"Say this is a test\");\n\nCompletions completions = client.getCompletions(\"{deploymentOrModelName}\", new CompletionsOptions(prompt));\n\nSystem.out.printf(\"Model ID=%s is created at %s.%n\", completions.getId(), completions.getCreatedAt());\nfor (Choice choice : completions.getChoices()) {\n  System.out.printf(\"Index: %d, Text: %s.%n\", choice.getIndex(), choice.getText());\n}\n```\n\nFor a complete sample example, see sample [Text Completions][sample_get_completions].\n\n### Streaming legacy completions\n\n```java readme-sample-getCompletionsStream\nList prompt = new ArrayList&lt;&gt;();\nprompt.add(\"How to bake a cake?\");\n\nIterableStream completionsStream = client\n  .getCompletionsStream(\"{deploymentOrModelName}\", new CompletionsOptions(prompt));\n\ncompletionsStream\n  .stream()\n  // Remove .skip(1) when using Non-Azure OpenAI API\n  // Note: the first chat completions can be ignored when using Azure OpenAI service which is a known service bug.\n  // TODO: remove .skip(1) when service fix the issue.\n  .skip(1)\n  .forEach(completions -&gt; System.out.print(completions.getChoices().get(0).getText()));\n```\n\nFor a complete sample example, see sample [Streaming Text Completions][sample_get_completions_streaming].\n\n### Chat completions\n\n``` java readme-sample-getChatCompletions\nList chatMessages = new ArrayList&lt;&gt;();\nchatMessages.add(new ChatRequestSystemMessage(\"You are a helpful assistant. You will talk like a pirate.\"));\nchatMessages.add(new ChatRequestUserMessage(\"Can you help me?\"));\nchatMessages.add(new ChatRequestAssistantMessage(\"Of course, me hearty! What can I do for ye?\"));\nchatMessages.add(new ChatRequestUserMessage(\"What's the best way to train a parrot?\"));\n\nChatCompletions chatCompletions = client.getChatCompletions(\"{deploymentOrModelName}\",\n  new ChatCompletionsOptions(chatMessages));\n\nSystem.out.printf(\"Model ID=%s is created at %s.%n\", chatCompletions.getId(), chatCompletions.getCreatedAt());\nfor (ChatChoice choice : chatCompletions.getChoices()) {\n  ChatResponseMessage message = choice.getMessage();\n  System.out.printf(\"Index: %d, Chat Role: %s.%n\", choice.getIndex(), message.getRole());\n  System.out.println(\"Message:\");\n  System.out.println(message.getContent());\n}",
                    "title": "Azure OpenAI client library for Java",
                    "url": "https://openainewsdkteststorage.blob.core.windows.net/fileupload-openai-readmes-index/azure-java.md",
                    "filepath": "azure-java.md",
                    "chunk_id": "1"
                  },
                  {
                    "content": "### Streaming Helpers\n\nThe SDK also includes helpers to process streams and handle the incoming events.\n\n```python\nwith client.beta.threads.runs.stream(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n  instructions=\"Please address the user as Jane Doe. The user has a premium account.\",\n) as stream:\n  for event in stream:\n  # Print the text from text delta events\n  if event.type == \"thread.message.delta\" and event.data.delta.content:\n  print(event.data.delta.content[0].text)\n```\n\nMore information on streaming helpers can be found in the dedicated documentation: [helpers.md](helpers.md)\n\n## Async usage\n\nSimply import `AsyncOpenAI` instead of `OpenAI` and use `await` with each API call:\n\n```python\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n  # This is the default and can be omitted\n  api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nasync def main() -&gt; None:\n  chat_completion = await client.chat.completions.create(\n  messages=[\n  {\n  \"role\": \"user\",\n  \"content\": \"Say this is a test\",\n  }\n  ],\n  model=\"gpt-3.5-turbo\",\n  )\n\nasyncio.run(main())\n```\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\n## Streaming responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n  model=\"gpt-4\",\n  messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n  stream=True,\n)\nfor chunk in stream:\n  print(chunk.choices[0].delta.content or \"\", end=\"\")\n```\n\nThe async client uses the exact same interface.\n\n```python\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nasync def main():\n  stream = await client.chat.completions.create(\n  model=\"gpt-4\",\n  messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n  stream=True,\n  )\n  async for chunk in stream:\n  print(chunk.choices[0].delta.content or \"\", end=\"\")\n\nasyncio.run(main())",
                    "title": "OpenAI Python API library",
                    "url": "https://openainewsdkteststorage.blob.core.windows.net/fileupload-openai-readmes-index/python.md",
                    "filepath": "python.md",
                    "chunk_id": "1"
                  },
                  {
                    "content": "## Semantic versioning\n\nThis package generally follows [SemVer](https://semver.org/spec/v2.0.0.html) conventions, though certain backwards-incompatible changes may be released as minor versions:\n\n1. Changes that only affect static types, without breaking runtime behavior.\n2. Changes to library internals which are technically public but not intended or documented for external use. _(Please open a GitHub issue to let us know if you are relying on such internals)_.\n3. Changes that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an [issue](https://www.github.com/openai/openai-node/issues) with questions, bugs, or suggestions.\n\n## Requirements\n\nTypeScript &gt;= 4.5 is supported.\n\nThe following runtimes are supported:\n\n- Node.js 18 LTS or later ([non-EOL](https://endoflife.date/nodejs)) versions.\n- Deno v1.28.0 or higher, using `import OpenAI from \"npm:openai\"`.\n- Bun 1.0 or later.\n- Cloudflare Workers.\n- Vercel Edge Runtime.\n- Jest 28 or greater with the `\"node\"` environment (`\"jsdom\"` is not supported at this time).\n- Nitro v2.6 or greater.\n\nNote that React Native is not supported at this time.\n\nIf you are interested in other runtime environments, please open or upvote an issue on GitHub.",
                    "title": "OpenAI Node API Library",
                    "url": "https://openainewsdkteststorage.blob.core.windows.net/fileupload-openai-readmes-index/javascript.md",
                    "filepath": "javascript.md",
                    "chunk_id": "2"
                  }
                ],
                "intent": "[\"What is the functionality of the OpenAI package?\", \"What does the OpenAI package do?\"]"
              }
            }
          }
        ],
        "usage": {
          "prompt_tokens": 6543,
          "completion_tokens": 290,
          "total_tokens": 6833
        }
      }
    }
  ],
  "Variables": {}
}
