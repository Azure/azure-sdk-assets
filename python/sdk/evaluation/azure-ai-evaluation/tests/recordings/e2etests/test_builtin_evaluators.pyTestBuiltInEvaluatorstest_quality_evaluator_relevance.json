{
  "Entries": [
    {
      "RequestUri": "https://Sanitized.api.cognitive.microsoft.com/openai/deployments/aoai-deployment/chat/completions?api-version=2024-04-01-preview",
      "RequestMethod": "POST",
      "RequestHeaders": {
        "Accept": "application/json",
        "Accept-Encoding": "gzip, deflate",
        "api-key": "Sanitized",
        "Connection": "close",
        "Content-Length": "3508",
        "Content-Type": "application/json",
        "ms-azure-ai-promptflow-called-from": "promptflow-core",
        "User-Agent": "AsyncAzureOpenAI/Python 1.50.2",
        "x-ms-useragent": "azure-ai-evaluation/1.0.0b5",
        "X-Stainless-Arch": "x64",
        "X-Stainless-Async": "async:asyncio",
        "X-Stainless-Lang": "python",
        "X-Stainless-OS": "Other:XXX",
        "X-Stainless-Package-Version": "vX.X.X",
        "x-stainless-retry-count": "0",
        "X-Stainless-Runtime": "CPython",
        "X-Stainless-Runtime-Version": "3.X.X"
      },
      "RequestBody": {
        "messages": [
          {
            "role": "system",
            "content": "You are an AI assistant. You will be given the definition of an evaluation metric for assessing the quality of an answer in a question-answering task. Your job is to compute an accurate evaluation score using the provided evaluation metric. You should return a single integer value between 1 to 5 representing the evaluation metric. You will include no other text or information."
          },
          {
            "role": "user",
            "content": "Relevance measures how well the answer addresses the main aspects of the question, based on the context. Consider whether all and only the important aspects are contained in the answer when evaluating relevance. Given the context and question, score the relevance of the answer between one to five stars using the following rating scale:\nOne star: the answer completely lacks relevance\nTwo stars: the answer mostly lacks relevance\nThree stars: the answer is partially relevant\nFour stars: the answer is mostly relevant\nFive stars: the answer has perfect relevance\n\nThis rating value should always be an integer between 1 and 5. So the rating produced should be 1 or 2 or 3 or 4 or 5.\n\ncontext: Marie Curie was a Polish-born physicist and chemist who pioneered research on radioactivity and was the first woman to win a Nobel Prize.\nquestion: What field did Marie Curie excel in?\nanswer: Marie Curie was a renowned painter who focused mainly on impressionist styles and techniques.\nstars: 1\n\ncontext: The Beatles were an English rock band formed in Liverpool in 1960, and they are widely regarded as the most influential music band in history.\nquestion: Where were The Beatles formed?\nanswer: The band The Beatles began their journey in London, England, and they changed the history of music.\nstars: 2\n\ncontext: The recent Mars rover, Perseverance, was launched in 2020 with the main goal of searching for signs of ancient life on Mars. The rover also carries an experiment called MOXIE, which aims to generate oxygen from the Martian atmosphere.\nquestion: What are the main goals of Perseverance Mars rover mission?\nanswer: The Perseverance Mars rover mission focuses on searching for signs of ancient life on Mars.\nstars: 3\n\ncontext: The Mediterranean diet is a commonly recommended dietary plan that emphasizes fruits, vegetables, whole grains, legumes, lean proteins, and healthy fats. Studies have shown that it offers numerous health benefits, including a reduced risk of heart disease and improved cognitive health.\nquestion: What are the main components of the Mediterranean diet?\nanswer: The Mediterranean diet primarily consists of fruits, vegetables, whole grains, and legumes.\nstars: 4\n\ncontext: The Queen's Royal Castle is a well-known tourist attraction in the United Kingdom. It spans over 500 acres and contains extensive gardens and parks. The castle was built in the 15th century and has been home to generations of royalty.\nquestion: What are the main attractions of the Queen's Royal Castle?\nanswer: The main attractions of the Queen's Royal Castle are its expansive 500-acre grounds, extensive gardens, parks, and the historical castle itself, which dates back to the 15th century and has housed generations of royalty.\nstars: 5\n\ncontext: Tokyo is Japan's capital.\nquestion: What is the capital of Japan?\nanswer: The capital of Japan is Tokyo.\nstars:"
          }
        ],
        "model": "aoai-deployment",
        "frequency_penalty": 0,
        "max_tokens": 1,
        "presence_penalty": 0,
        "response_format": {
          "type": "text"
        },
        "temperature": 0.0,
        "top_p": 1.0
      },
      "StatusCode": 429,
      "ResponseHeaders": {
        "apim-request-id": "1b700c59-9bd3-482b-9e3b-2d9dd8a68fbc",
        "Connection": "close",
        "Content-Length": "347",
        "Content-Type": "application/json",
        "Date": "Thu, 24 Oct 2024 18:01:57 GMT",
        "policy-id": "DeploymentRatelimit-Call",
        "Retry-After": "4",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
        "X-Content-Type-Options": "nosniff",
        "x-ms-region": "East US",
        "x-ratelimit-reset-requests": "4"
      },
      "ResponseBody": {
        "error": {
          "code": "429",
          "message": "Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-04-01-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 4 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit."
        }
      }
    },
    {
      "RequestUri": "https://Sanitized.api.cognitive.microsoft.com/openai/deployments/aoai-deployment/chat/completions?api-version=2024-04-01-preview",
      "RequestMethod": "POST",
      "RequestHeaders": {
        "Accept": "application/json",
        "Accept-Encoding": "gzip, deflate",
        "api-key": "Sanitized",
        "Connection": "close",
        "Content-Length": "3508",
        "Content-Type": "application/json",
        "ms-azure-ai-promptflow-called-from": "promptflow-core",
        "User-Agent": "AsyncAzureOpenAI/Python 1.50.2",
        "x-ms-useragent": "azure-ai-evaluation/1.0.0b5",
        "X-Stainless-Arch": "x64",
        "X-Stainless-Async": "async:asyncio",
        "X-Stainless-Lang": "python",
        "X-Stainless-OS": "Other:XXX",
        "X-Stainless-Package-Version": "vX.X.X",
        "x-stainless-retry-count": "0",
        "X-Stainless-Runtime": "CPython",
        "X-Stainless-Runtime-Version": "3.X.X"
      },
      "RequestBody": {
        "messages": [
          {
            "role": "system",
            "content": "You are an AI assistant. You will be given the definition of an evaluation metric for assessing the quality of an answer in a question-answering task. Your job is to compute an accurate evaluation score using the provided evaluation metric. You should return a single integer value between 1 to 5 representing the evaluation metric. You will include no other text or information."
          },
          {
            "role": "user",
            "content": "Relevance measures how well the answer addresses the main aspects of the question, based on the context. Consider whether all and only the important aspects are contained in the answer when evaluating relevance. Given the context and question, score the relevance of the answer between one to five stars using the following rating scale:\nOne star: the answer completely lacks relevance\nTwo stars: the answer mostly lacks relevance\nThree stars: the answer is partially relevant\nFour stars: the answer is mostly relevant\nFive stars: the answer has perfect relevance\n\nThis rating value should always be an integer between 1 and 5. So the rating produced should be 1 or 2 or 3 or 4 or 5.\n\ncontext: Marie Curie was a Polish-born physicist and chemist who pioneered research on radioactivity and was the first woman to win a Nobel Prize.\nquestion: What field did Marie Curie excel in?\nanswer: Marie Curie was a renowned painter who focused mainly on impressionist styles and techniques.\nstars: 1\n\ncontext: The Beatles were an English rock band formed in Liverpool in 1960, and they are widely regarded as the most influential music band in history.\nquestion: Where were The Beatles formed?\nanswer: The band The Beatles began their journey in London, England, and they changed the history of music.\nstars: 2\n\ncontext: The recent Mars rover, Perseverance, was launched in 2020 with the main goal of searching for signs of ancient life on Mars. The rover also carries an experiment called MOXIE, which aims to generate oxygen from the Martian atmosphere.\nquestion: What are the main goals of Perseverance Mars rover mission?\nanswer: The Perseverance Mars rover mission focuses on searching for signs of ancient life on Mars.\nstars: 3\n\ncontext: The Mediterranean diet is a commonly recommended dietary plan that emphasizes fruits, vegetables, whole grains, legumes, lean proteins, and healthy fats. Studies have shown that it offers numerous health benefits, including a reduced risk of heart disease and improved cognitive health.\nquestion: What are the main components of the Mediterranean diet?\nanswer: The Mediterranean diet primarily consists of fruits, vegetables, whole grains, and legumes.\nstars: 4\n\ncontext: The Queen's Royal Castle is a well-known tourist attraction in the United Kingdom. It spans over 500 acres and contains extensive gardens and parks. The castle was built in the 15th century and has been home to generations of royalty.\nquestion: What are the main attractions of the Queen's Royal Castle?\nanswer: The main attractions of the Queen's Royal Castle are its expansive 500-acre grounds, extensive gardens, parks, and the historical castle itself, which dates back to the 15th century and has housed generations of royalty.\nstars: 5\n\ncontext: Tokyo is Japan's capital.\nquestion: What is the capital of Japan?\nanswer: The capital of Japan is Tokyo.\nstars:"
          }
        ],
        "model": "aoai-deployment",
        "frequency_penalty": 0,
        "max_tokens": 1,
        "presence_penalty": 0,
        "response_format": {
          "type": "text"
        },
        "temperature": 0.0,
        "top_p": 1.0
      },
      "StatusCode": 200,
      "ResponseHeaders": {
        "Access-Control-Allow-Origin": "*",
        "apim-request-id": "fdeccf7e-abd8-4b20-b884-dfc180119bc6",
        "azureml-model-session": "d051-20240925115107",
        "Cache-Control": "must-revalidate, no-cache",
        "cmp-upstream-response-duration": "253",
        "Connection": "close",
        "Content-Length": "772",
        "Content-Type": "application/json",
        "Date": "Thu, 24 Oct 2024 18:02:02 GMT",
        "ms-azureml-model-time": "303",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
        "x-accel-buffering": "no",
        "X-Content-Type-Options": "nosniff",
        "x-envoy-upstream-service-time": "308",
        "x-ms-rai-invoked": "true",
        "x-ms-region": "East US",
        "x-ratelimit-remaining-requests": "0",
        "x-ratelimit-remaining-tokens": "9989",
        "X-Request-ID": "35ce005c-3697-4869-93a7-bc4c4e08a49d"
      },
      "ResponseBody": {
        "choices": [
          {
            "content_filter_results": {
              "hate": {
                "filtered": false,
                "severity": "safe"
              },
              "self_harm": {
                "filtered": false,
                "severity": "safe"
              },
              "sexual": {
                "filtered": false,
                "severity": "safe"
              },
              "violence": {
                "filtered": false,
                "severity": "safe"
              }
            },
            "finish_reason": "length",
            "index": 0,
            "logprobs": null,
            "message": {
              "content": "5",
              "role": "assistant"
            }
          }
        ],
        "created": 1729792922,
        "id": "Sanitized",
        "model": "aoai-deployment",
        "object": "chat.completion",
        "prompt_filter_results": [
          {
            "prompt_index": 0,
            "content_filter_results": {
              "hate": {
                "filtered": false,
                "severity": "safe"
              },
              "self_harm": {
                "filtered": false,
                "severity": "safe"
              },
              "sexual": {
                "filtered": false,
                "severity": "safe"
              },
              "violence": {
                "filtered": false,
                "severity": "safe"
              }
            }
          }
        ],
        "system_fingerprint": null,
        "usage": {
          "completion_tokens": 1,
          "prompt_tokens": 683,
          "total_tokens": 684
        }
      }
    },
    {
      "RequestUri": "https://Sanitized.api.cognitive.microsoft.com/openai/deployments/aoai-deployment/chat/completions?api-version=2024-04-01-preview",
      "RequestMethod": "POST",
      "RequestHeaders": {
        "Accept": "application/json",
        "Accept-Encoding": "gzip, deflate",
        "api-key": "Sanitized",
        "Connection": "close",
        "Content-Length": "3614",
        "Content-Type": "application/json",
        "ms-azure-ai-promptflow-called-from": "promptflow-core",
        "User-Agent": "AsyncAzureOpenAI/Python 1.50.2",
        "x-ms-useragent": "azure-ai-evaluation/1.0.0b5",
        "X-Stainless-Arch": "x64",
        "X-Stainless-Async": "async:asyncio",
        "X-Stainless-Lang": "python",
        "X-Stainless-OS": "Other:XXX",
        "X-Stainless-Package-Version": "vX.X.X",
        "x-stainless-retry-count": "0",
        "X-Stainless-Runtime": "CPython",
        "X-Stainless-Runtime-Version": "3.X.X"
      },
      "RequestBody": {
        "messages": [
          {
            "role": "system",
            "content": "You are an AI assistant. You will be given the definition of an evaluation metric for assessing the quality of an answer in a question-answering task. Your job is to compute an accurate evaluation score using the provided evaluation metric. You should return a single integer value between 1 to 5 representing the evaluation metric. You will include no other text or information."
          },
          {
            "role": "user",
            "content": "Relevance measures how well the answer addresses the main aspects of the question, based on the context. Consider whether all and only the important aspects are contained in the answer when evaluating relevance. Given the context and question, score the relevance of the answer between one to five stars using the following rating scale:\nOne star: the answer completely lacks relevance\nTwo stars: the answer mostly lacks relevance\nThree stars: the answer is partially relevant\nFour stars: the answer is mostly relevant\nFive stars: the answer has perfect relevance\n\nThis rating value should always be an integer between 1 and 5. So the rating produced should be 1 or 2 or 3 or 4 or 5.\n\ncontext: Marie Curie was a Polish-born physicist and chemist who pioneered research on radioactivity and was the first woman to win a Nobel Prize.\nquestion: What field did Marie Curie excel in?\nanswer: Marie Curie was a renowned painter who focused mainly on impressionist styles and techniques.\nstars: 1\n\ncontext: The Beatles were an English rock band formed in Liverpool in 1960, and they are widely regarded as the most influential music band in history.\nquestion: Where were The Beatles formed?\nanswer: The band The Beatles began their journey in London, England, and they changed the history of music.\nstars: 2\n\ncontext: The recent Mars rover, Perseverance, was launched in 2020 with the main goal of searching for signs of ancient life on Mars. The rover also carries an experiment called MOXIE, which aims to generate oxygen from the Martian atmosphere.\nquestion: What are the main goals of Perseverance Mars rover mission?\nanswer: The Perseverance Mars rover mission focuses on searching for signs of ancient life on Mars.\nstars: 3\n\ncontext: The Mediterranean diet is a commonly recommended dietary plan that emphasizes fruits, vegetables, whole grains, legumes, lean proteins, and healthy fats. Studies have shown that it offers numerous health benefits, including a reduced risk of heart disease and improved cognitive health.\nquestion: What are the main components of the Mediterranean diet?\nanswer: The Mediterranean diet primarily consists of fruits, vegetables, whole grains, and legumes.\nstars: 4\n\ncontext: The Queen's Royal Castle is a well-known tourist attraction in the United Kingdom. It spans over 500 acres and contains extensive gardens and parks. The castle was built in the 15th century and has been home to generations of royalty.\nquestion: What are the main attractions of the Queen's Royal Castle?\nanswer: The main attractions of the Queen's Royal Castle are its expansive 500-acre grounds, extensive gardens, parks, and the historical castle itself, which dates back to the 15th century and has housed generations of royalty.\nstars: 5\n\ncontext: {'global_context': 'Global context', 'query_context': 'Customer wants to know the capital of France', 'response_context': 'Paris is the capital of France'}\nquestion: What is the capital of France?\nanswer: Paris\nstars:"
          }
        ],
        "model": "aoai-deployment",
        "frequency_penalty": 0,
        "max_tokens": 1,
        "presence_penalty": 0,
        "response_format": {
          "type": "text"
        },
        "temperature": 0.0,
        "top_p": 1.0
      },
      "StatusCode": 200,
      "ResponseHeaders": {
        "Access-Control-Allow-Origin": "*",
        "apim-request-id": "b44a0098-9bea-4411-bec8-b548a6b1c150",
        "azureml-model-session": "d051-20240925115107",
        "Cache-Control": "must-revalidate, no-cache",
        "cmp-upstream-response-duration": "170",
        "Connection": "close",
        "Content-Length": "772",
        "Content-Type": "application/json",
        "Date": "Thu, 24 Oct 2024 18:02:02 GMT",
        "ms-azureml-model-time": "212",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
        "x-accel-buffering": "no",
        "X-Content-Type-Options": "nosniff",
        "x-envoy-upstream-service-time": "216",
        "x-ms-rai-invoked": "true",
        "x-ms-region": "East US",
        "x-ratelimit-remaining-requests": "0",
        "x-ratelimit-remaining-tokens": "9988",
        "X-Request-ID": "ca1e0b2e-faf7-4d0f-b25e-19dac4f67a41"
      },
      "ResponseBody": {
        "choices": [
          {
            "content_filter_results": {
              "hate": {
                "filtered": false,
                "severity": "safe"
              },
              "self_harm": {
                "filtered": false,
                "severity": "safe"
              },
              "sexual": {
                "filtered": false,
                "severity": "safe"
              },
              "violence": {
                "filtered": false,
                "severity": "safe"
              }
            },
            "finish_reason": "length",
            "index": 0,
            "logprobs": null,
            "message": {
              "content": "5",
              "role": "assistant"
            }
          }
        ],
        "created": 1729792922,
        "id": "Sanitized",
        "model": "aoai-deployment",
        "object": "chat.completion",
        "prompt_filter_results": [
          {
            "prompt_index": 0,
            "content_filter_results": {
              "hate": {
                "filtered": false,
                "severity": "safe"
              },
              "self_harm": {
                "filtered": false,
                "severity": "safe"
              },
              "sexual": {
                "filtered": false,
                "severity": "safe"
              },
              "violence": {
                "filtered": false,
                "severity": "safe"
              }
            }
          }
        ],
        "system_fingerprint": null,
        "usage": {
          "completion_tokens": 1,
          "prompt_tokens": 706,
          "total_tokens": 707
        }
      }
    },
    {
      "RequestUri": "https://Sanitized.api.cognitive.microsoft.com/openai/deployments/aoai-deployment/chat/completions?api-version=2024-04-01-preview",
      "RequestMethod": "POST",
      "RequestHeaders": {
        "Accept": "application/json",
        "Accept-Encoding": "gzip, deflate",
        "api-key": "Sanitized",
        "Connection": "close",
        "Content-Length": "3620",
        "Content-Type": "application/json",
        "ms-azure-ai-promptflow-called-from": "promptflow-core",
        "User-Agent": "AsyncAzureOpenAI/Python 1.50.2",
        "x-ms-useragent": "azure-ai-evaluation/1.0.0b5",
        "X-Stainless-Arch": "x64",
        "X-Stainless-Async": "async:asyncio",
        "X-Stainless-Lang": "python",
        "X-Stainless-OS": "Other:XXX",
        "X-Stainless-Package-Version": "vX.X.X",
        "x-stainless-retry-count": "0",
        "X-Stainless-Runtime": "CPython",
        "X-Stainless-Runtime-Version": "3.X.X"
      },
      "RequestBody": {
        "messages": [
          {
            "role": "system",
            "content": "You are an AI assistant. You will be given the definition of an evaluation metric for assessing the quality of an answer in a question-answering task. Your job is to compute an accurate evaluation score using the provided evaluation metric. You should return a single integer value between 1 to 5 representing the evaluation metric. You will include no other text or information."
          },
          {
            "role": "user",
            "content": "Relevance measures how well the answer addresses the main aspects of the question, based on the context. Consider whether all and only the important aspects are contained in the answer when evaluating relevance. Given the context and question, score the relevance of the answer between one to five stars using the following rating scale:\nOne star: the answer completely lacks relevance\nTwo stars: the answer mostly lacks relevance\nThree stars: the answer is partially relevant\nFour stars: the answer is mostly relevant\nFive stars: the answer has perfect relevance\n\nThis rating value should always be an integer between 1 and 5. So the rating produced should be 1 or 2 or 3 or 4 or 5.\n\ncontext: Marie Curie was a Polish-born physicist and chemist who pioneered research on radioactivity and was the first woman to win a Nobel Prize.\nquestion: What field did Marie Curie excel in?\nanswer: Marie Curie was a renowned painter who focused mainly on impressionist styles and techniques.\nstars: 1\n\ncontext: The Beatles were an English rock band formed in Liverpool in 1960, and they are widely regarded as the most influential music band in history.\nquestion: Where were The Beatles formed?\nanswer: The band The Beatles began their journey in London, England, and they changed the history of music.\nstars: 2\n\ncontext: The recent Mars rover, Perseverance, was launched in 2020 with the main goal of searching for signs of ancient life on Mars. The rover also carries an experiment called MOXIE, which aims to generate oxygen from the Martian atmosphere.\nquestion: What are the main goals of Perseverance Mars rover mission?\nanswer: The Perseverance Mars rover mission focuses on searching for signs of ancient life on Mars.\nstars: 3\n\ncontext: The Mediterranean diet is a commonly recommended dietary plan that emphasizes fruits, vegetables, whole grains, legumes, lean proteins, and healthy fats. Studies have shown that it offers numerous health benefits, including a reduced risk of heart disease and improved cognitive health.\nquestion: What are the main components of the Mediterranean diet?\nanswer: The Mediterranean diet primarily consists of fruits, vegetables, whole grains, and legumes.\nstars: 4\n\ncontext: The Queen's Royal Castle is a well-known tourist attraction in the United Kingdom. It spans over 500 acres and contains extensive gardens and parks. The castle was built in the 15th century and has been home to generations of royalty.\nquestion: What are the main attractions of the Queen's Royal Castle?\nanswer: The main attractions of the Queen's Royal Castle are its expansive 500-acre grounds, extensive gardens, parks, and the historical castle itself, which dates back to the 15th century and has housed generations of royalty.\nstars: 5\n\ncontext: {'global_context': 'Global context', 'query_context': 'Customer wants to know the capital of Hawaii', 'response_context': 'Honolulu is the capital of Hawaii'}\nquestion: What is the capital of Hawaii?\nanswer: Honolulu\nstars:"
          }
        ],
        "model": "aoai-deployment",
        "frequency_penalty": 0,
        "max_tokens": 1,
        "presence_penalty": 0,
        "response_format": {
          "type": "text"
        },
        "temperature": 0.0,
        "top_p": 1.0
      },
      "StatusCode": 200,
      "ResponseHeaders": {
        "Access-Control-Allow-Origin": "*",
        "apim-request-id": "204e1f0c-c38d-47bb-a49b-f7d1e07a50d3",
        "azureml-model-session": "d051-20240925115107",
        "Cache-Control": "must-revalidate, no-cache",
        "cmp-upstream-response-duration": "166",
        "Connection": "close",
        "Content-Length": "772",
        "Content-Type": "application/json",
        "Date": "Thu, 24 Oct 2024 18:02:03 GMT",
        "ms-azureml-model-time": "215",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
        "x-accel-buffering": "no",
        "X-Content-Type-Options": "nosniff",
        "x-envoy-upstream-service-time": "219",
        "x-ms-rai-invoked": "true",
        "x-ms-region": "East US",
        "x-ratelimit-remaining-requests": "0",
        "x-ratelimit-remaining-tokens": "9987",
        "X-Request-ID": "07affa9c-f961-4daf-b557-64113da1c45a"
      },
      "ResponseBody": {
        "choices": [
          {
            "content_filter_results": {
              "hate": {
                "filtered": false,
                "severity": "safe"
              },
              "self_harm": {
                "filtered": false,
                "severity": "safe"
              },
              "sexual": {
                "filtered": false,
                "severity": "safe"
              },
              "violence": {
                "filtered": false,
                "severity": "safe"
              }
            },
            "finish_reason": "length",
            "index": 0,
            "logprobs": null,
            "message": {
              "content": "5",
              "role": "assistant"
            }
          }
        ],
        "created": 1729792923,
        "id": "Sanitized",
        "model": "aoai-deployment",
        "object": "chat.completion",
        "prompt_filter_results": [
          {
            "prompt_index": 0,
            "content_filter_results": {
              "hate": {
                "filtered": false,
                "severity": "safe"
              },
              "self_harm": {
                "filtered": false,
                "severity": "safe"
              },
              "sexual": {
                "filtered": false,
                "severity": "safe"
              },
              "violence": {
                "filtered": false,
                "severity": "safe"
              }
            }
          }
        ],
        "system_fingerprint": null,
        "usage": {
          "completion_tokens": 1,
          "prompt_tokens": 707,
          "total_tokens": 708
        }
      }
    }
  ],
  "Variables": {}
}
