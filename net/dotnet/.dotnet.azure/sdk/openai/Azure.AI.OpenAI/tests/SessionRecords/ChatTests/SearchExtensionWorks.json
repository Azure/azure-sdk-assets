{
  "Entries": [
    {
      "RequestUri": "https://Sanitized.openai.azure.com/openai/deployments/gpt-4o-0806/chat/completions?api-version=2025-04-01-preview",
      "RequestMethod": "POST",
      "RequestHeaders": {
        "Accept": "application/json",
        "api-key": "Sanitized",
        "Content-Length": "350",
        "Content-Type": "application/json",
        "User-Agent": "azsdk-net-AI.OpenAI/2.2.0-beta.4 (.NET 8.0.15; Microsoft Windows 10.0.26100)",
        "x-ms-client-request-id": "Sanitized"
      },
      "RequestBody": {
        "messages": [
          {
            "role": "user",
            "content": "What does the term 'PR complete' mean?"
          }
        ],
        "model": "gpt-4o-0806",
        "data_sources": [
          {
            "type": "azure_search",
            "parameters": {
              "allow_partial_result": true,
              "endpoint": "https://Sanitized.search.windows.net/",
              "index_name": "openai-readmes-index",
              "authentication": {
                "type": "api_key",
                "key": "Sanitized"
              },
              "query_type": "simple"
            }
          }
        ]
      },
      "StatusCode": 200,
      "ResponseHeaders": {
        "apim-request-id": "b2de23b0-b349-4610-afa0-435b04d6efc6",
        "Connection": "keep-alive",
        "Content-Length": "15289",
        "Content-Type": "application/json; charset=UTF-8",
        "Date": "Fri, 25 Apr 2025 18:08:02 GMT",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
        "X-Content-Type-Options": "nosniff",
        "x-envoy-upstream-service-time": "2022",
        "x-ms-client-request-id": "Sanitized",
        "x-ms-deployment-name": "gpt-4o-0806",
        "x-ms-region": "East US",
        "x-ratelimit-limit-requests": "4490",
        "x-ratelimit-limit-tokens": "449000",
        "x-ratelimit-remaining-requests": "4457",
        "x-ratelimit-remaining-tokens": "430740"
      },
      "ResponseBody": {
        "id": "3e75c0ea-ab28-485b-b339-6e21947a506e",
        "model": "gpt-4o",
        "created": 1745604482,
        "object": "extensions.chat.completion",
        "choices": [
          {
            "index": 0,
            "finish_reason": "stop",
            "message": {
              "role": "assistant",
              "content": "The requested information is not available in the retrieved data. Please try another query or topic.",
              "end_turn": true,
              "context": {
                "citations": [
                  {
                    "content": "## Key concepts\n\n## Examples\nThe following sections provide several code snippets covering some of the most common OpenAI service tasks, including:\n\n* [Text completions sample](#text-completions \"Text completions\")\n* [Streaming text completions sample](#streaming-text-completions \"Streaming text completions\")\n* [Chat completions sample](#chat-completions \"Chat completions\")\n* [Streaming chat completions sample](#streaming-chat-completions \"Streaming chat completions\")\n* [Embeddings sample](#text-embeddings \"Text Embeddings\")\n* [Image Generation sample](#image-generation \"Image Generation\")\n* [Audio Transcription sample](#audio-transcription \"Audio Transcription\")\n* [Audio Translation sample](#audio-translation \"Audio Translation\")\n\n### Legacy completions\n\nIt is generally preferable to use Chat Completions instead. However, Completions are still supported:\n\n``` java readme-sample-getCompletions\nList prompt = new ArrayList&lt;&gt;();\nprompt.add(\"Say this is a test\");\n\nCompletions completions = client.getCompletions(\"{deploymentOrModelName}\", new CompletionsOptions(prompt));\n\nSystem.out.printf(\"Model ID=%s is created at %s.%n\", completions.getId(), completions.getCreatedAt());\nfor (Choice choice : completions.getChoices()) {\n  System.out.printf(\"Index: %d, Text: %s.%n\", choice.getIndex(), choice.getText());\n}\n```\n\nFor a complete sample example, see sample [Text Completions][sample_get_completions].\n\n### Streaming legacy completions\n\n```java readme-sample-getCompletionsStream\nList prompt = new ArrayList&lt;&gt;();\nprompt.add(\"How to bake a cake?\");\n\nIterableStream completionsStream = client\n  .getCompletionsStream(\"{deploymentOrModelName}\", new CompletionsOptions(prompt));\n\ncompletionsStream\n  .stream()\n  // Remove .skip(1) when using Non-Azure OpenAI API\n  // Note: the first chat completions can be ignored when using Azure OpenAI service which is a known service bug.\n  // TODO: remove .skip(1) when service fix the issue.\n  .skip(1)\n  .forEach(completions -&gt; System.out.print(completions.getChoices().get(0).getText()));\n```\n\nFor a complete sample example, see sample [Streaming Text Completions][sample_get_completions_streaming].\n\n### Chat completions\n\n``` java readme-sample-getChatCompletions\nList chatMessages = new ArrayList&lt;&gt;();\nchatMessages.add(new ChatRequestSystemMessage(\"You are a helpful assistant. You will talk like a pirate.\"));\nchatMessages.add(new ChatRequestUserMessage(\"Can you help me?\"));\nchatMessages.add(new ChatRequestAssistantMessage(\"Of course, me hearty! What can I do for ye?\"));\nchatMessages.add(new ChatRequestUserMessage(\"What's the best way to train a parrot?\"));\n\nChatCompletions chatCompletions = client.getChatCompletions(\"{deploymentOrModelName}\",\n  new ChatCompletionsOptions(chatMessages));\n\nSystem.out.printf(\"Model ID=%s is created at %s.%n\", chatCompletions.getId(), chatCompletions.getCreatedAt());\nfor (ChatChoice choice : chatCompletions.getChoices()) {\n  ChatResponseMessage message = choice.getMessage();\n  System.out.printf(\"Index: %d, Chat Role: %s.%n\", choice.getIndex(), message.getRole());\n  System.out.println(\"Message:\");\n  System.out.println(message.getContent());\n}",
                    "title": "Azure OpenAI client library for Java",
                    "url": "https://openainewsdkteststorage.blob.core.windows.net/fileupload-openai-readmes-index/azure-java.md",
                    "filepath": "azure-java.md",
                    "chunk_id": "0"
                  },
                  {
                    "content": "For a complete sample example, see sample [Chat Completions][sample_get_chat_completions].\n\nFor `function call` sample, see [function call][sample_chat_completion_function_call]. However, they are considered \na legacy feature. Using tools is the preferred way. For more details see sample [tool calls][sample_tool_calls].\n\nFor `Bring Your Own Data` sample, see [Bring Your Own Data][sample_chat_completion_function_call].\n\nPlease refer to the service documentation for a conceptual discussion of [text completion][microsoft_docs_openai_completion].\n\n### Streaming chat completions\n\n```java readme-sample-getChatCompletionsStream\nList chatMessages = new ArrayList&lt;&gt;();\nchatMessages.add(new ChatRequestSystemMessage(\"You are a helpful assistant. You will talk like a pirate.\"));\nchatMessages.add(new ChatRequestUserMessage(\"Can you help me?\"));\nchatMessages.add(new ChatRequestAssistantMessage(\"Of course, me hearty! What can I do for ye?\"));\nchatMessages.add(new ChatRequestUserMessage(\"What's the best way to train a parrot?\"));\n\nclient.getChatCompletionsStream(\"{deploymentOrModelName}\", new ChatCompletionsOptions(chatMessages))\n  .forEach(chatCompletions -&gt; {\n  if (CoreUtils.isNullOrEmpty(chatCompletions.getChoices())) {\n  return;\n  }\n  ChatResponseMessage delta = chatCompletions.getChoices().get(0).getDelta();\n  if (delta.getRole() != null) {\n  System.out.println(\"Role = \" + delta.getRole());\n  }\n  if (delta.getContent() != null) {\n  String content = delta.getContent();\n  System.out.print(content);\n  }\n  });\n```\n\nTo compute tokens in streaming chat completions, see sample [Streaming Chat Completions][sample_get_chat_completions_streaming].\n\n### Text embeddings\n\n```java readme-sample-getEmbedding\nEmbeddingsOptions embeddingsOptions = new EmbeddingsOptions(\n  Arrays.asList(\"Your text string goes here\"));\n\nEmbeddings embeddings = client.getEmbeddings(\"{deploymentOrModelName}\", embeddingsOptions);\n\nfor (EmbeddingItem item : embeddings.getData()) {\n  System.out.printf(\"Index: %d.%n\", item.getPromptIndex());\n  for (Float embedding : item.getEmbedding()) {\n  System.out.printf(\"%f;\", embedding);\n  }\n}\n```\nFor a complete sample example, see sample [Embedding][sample_get_embedding].\n\nPlease refer to the service documentation for a conceptual discussion of [openAI embedding][microsoft_docs_openai_embedding].\n\n### Image Generation\n\n```java readme-sample-imageGeneration\nImageGenerationOptions imageGenerationOptions = new ImageGenerationOptions(\n  \"A drawing of the Seattle skyline in the style of Van Gogh\");\nImageGenerations images = client.getImageGenerations(\"{deploymentOrModelName}\", imageGenerationOptions);\n\nfor (ImageGenerationData imageGenerationData : images.getData()) {\n  System.out.printf(\n  \"Image location URL that provides temporary access to download the generated image is %s.%n\",\n  imageGenerationData.getUrl());\n}",
                    "title": "Azure OpenAI client library for Java",
                    "url": "https://openainewsdkteststorage.blob.core.windows.net/fileupload-openai-readmes-index/azure-java.md",
                    "filepath": "azure-java.md",
                    "chunk_id": "1"
                  },
                  {
                    "content": "# OpenAI Python API library\n\n[![PyPI version](https://img.shields.io/pypi/v/openai.svg)](https://pypi.org/project/openai/)\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+\napplication. The library includes type definitions for all request params and response fields,\nand offers both synchronous and asynchronous clients powered by [httpx](https://github.com/encode/httpx).\n\nIt is generated from our [OpenAPI specification](https://github.com/openai/openai-openapi) with [Stainless](https://stainlessapi.com/).\n\n## Documentation\n\nThe REST API documentation can be found [on platform.openai.com](https://platform.openai.com/docs). The full API of this library can be found in [api.md](api.md).\n\n## Installation\n\n&gt; [!IMPORTANT]\n&gt; The SDK was rewritten in v1, which was released November 6th 2023. See the [v1 migration guide](https://github.com/openai/openai-python/discussions/742), which includes scripts to automatically update your code.\n\n```sh\n# install from PyPI\npip install openai\n```\n\n## Usage\n\nThe full API of this library can be found in [api.md](api.md).\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n  # This is the default and can be omitted\n  api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n  messages=[\n  {\n  \"role\": \"user\",\n  \"content\": \"Say this is a test\",\n  }\n  ],\n  model=\"gpt-3.5-turbo\",\n)\n```\n\nWhile you can provide an `api_key` keyword argument,\nwe recommend using [python-dotenv](https://pypi.org/project/python-dotenv/)\nto add `OPENAI_API_KEY=\"My API Key\"` to your `.env` file\nso that your API Key is not stored in source control.\n\n### Polling Helpers\n\nWhen interacting with the API some actions such as starting a Run and adding files to vector stores are asynchronous and take time to complete. The SDK includes\nhelper functions which will poll the status until it reaches a terminal state and then return the resulting object.\nIf an API method results in an action which could benefit from polling there will be a corresponding version of the\nmethod ending in '\\_and_poll'.\n\nFor instance to create a Run and poll until it reaches a terminal state you can run:\n\n```python\nrun = client.beta.threads.runs.create_and_poll(\n  thread_id=thread.id,\n  assistant_id=assistant.id,\n)\n```\n\nMore information on the lifecycle of a Run can be found in the [Run Lifecycle Documentation](https://platform.openai.com/docs/assistants/how-it-works/run-lifecycle)\n\n### Bulk Upload Helpers\n\nWhen creating an interacting with vector stores, you can use the polling helpers to monitor the status of operations.\nFor convenience, we also provide a bulk upload helper to allow you to simultaneously upload several files at once.\n\n```python\nsample_files = [Path(\"sample-paper.pdf\"), ...]\n\nbatch = await client.vector_stores.file_batches.upload_and_poll(\n  store.id,\n  files=sample_files,\n)",
                    "title": "OpenAI Python API library",
                    "url": "https://openainewsdkteststorage.blob.core.windows.net/fileupload-openai-readmes-index/python.md",
                    "filepath": "python.md",
                    "chunk_id": "0"
                  },
                  {
                    "content": "### Text To Speech\n\nThe OpenAI service starts supporting `text to speech` with the introduction of `tts` models.\nThe following code snippet shows how to use the service to convert text to speech.\n```java readme-sample-textToSpeech\nString deploymentOrModelId = \"{azure-open-ai-deployment-model-id}\";\nSpeechGenerationOptions options = new SpeechGenerationOptions(\n  \"Today is a wonderful day to build something people love!\",\n  SpeechVoice.ALLOY);\nBinaryData speech = client.generateSpeechFromText(deploymentOrModelId, options);\n// Checkout your generated speech in the file system.\nPath path = Paths.get(\"{your-local-file-path}/speech.wav\");\nFiles.write(path, speech.toBytes());See sample [Text to Speech][sample_text_to_speech] for a complete sample.\nPlease refer to the service documentation for a conceptual discussion of [Text to Speech][microsoft_docs_text_to_speech].\n\n## Troubleshooting\n### Enable client logging\nYou can set the `AZURE_LOG_LEVEL` environment variable to view logging statements made in the client library. For\nexample, setting `AZURE_LOG_LEVEL=2` would show all informational, warning, and error log messages. The log levels can\nbe found here: [log levels][logLevels].\n\n### Default HTTP Client\nAll client libraries by default use the Netty HTTP client. Adding the above dependency will automatically configure\nthe client library to use the Netty HTTP client. Configuring or changing the HTTP client is detailed in the\n[HTTP clients wiki](https://github.com/Azure/azure-sdk-for-java/wiki/HTTP-clients).\n\n### Default SSL library\nAll client libraries, by default, use the Tomcat-native Boring SSL library to enable native-level performance for SSL\noperations. The Boring SSL library is an uber jar containing native libraries for Linux / macOS / Windows, and provides\nbetter performance compared to the default SSL implementation within the JDK. For more information, including how to\nreduce the dependency size, refer to the [performance tuning][performance_tuning] section of the wiki.\n\nFor more details, see [TROUBLESHOOTING][troubleshooting] guideline.\n\n## Next steps\n- Samples are explained in detail [here][samples_readme].\n\n## Contributing\n\nFor details on contributing to this repository, see the [contributing guide](https://github.com/Azure/azure-sdk-for-java/blob/main/CONTRIBUTING.md).\n\n1. Fork it\n1. Create your feature branch (`git checkout -b my-new-feature`)\n1. Commit your changes (`git commit -am 'Add some feature'`)\n1. Push to the branch (`git push origin my-new-feature`)\n1. Create new Pull Request",
                    "title": "Azure OpenAI client library for Java",
                    "url": "https://openainewsdkteststorage.blob.core.windows.net/fileupload-openai-readmes-index/azure-java.md",
                    "filepath": "azure-java.md",
                    "chunk_id": "2"
                  },
                  {
                    "content": "## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a [Contributor License Agreement (CLA)][cla] declaring that you have the right to, and actually do, grant us the rights to use your contribution.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate\nthe PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to\ndo this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct][coc]. For more information, see\nthe [Code of Conduct FAQ][coc_faq] or contact [opencode@microsoft.com][coc_contact] with any additional questions or\ncomments.\n\n\n[azure_openai_access]: https://learn.microsoft.com/azure/cognitive-services/openai/overview#how-do-i-get-access-to-azure-openai\n[azopenai_repo]: https://github.com/Azure/azure-sdk-for-go/tree/main/sdk/ai/azopenai\n[azopenai_pkg_go]: https://pkg.go.dev/github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai\n[azure_identity]: https://pkg.go.dev/github.com/Azure/azure-sdk-for-go/sdk/azidentity\n[azure_sub]: https://azure.microsoft.com/free/\n[openai_docs]: https://learn.microsoft.com/azure/cognitive-services/openai\n[openai_key_concepts]: https://learn.microsoft.com/azure/cognitive-services/openai/overview#key-concepts\n[openai_rest_docs]: https://learn.microsoft.com/azure/cognitive-services/openai/reference\n[cla]: https://cla.microsoft.com\n[coc]: https://opensource.microsoft.com/codeofconduct/\n[coc_faq]: https://opensource.microsoft.com/codeofconduct/faq/\n[coc_contact]: mailto:opencode@microsoft.com\n[azure_openai_quickstart]: https://learn.microsoft.com/azure/cognitive-services/openai/quickstart",
                    "title": "Azure OpenAI client module for Go",
                    "url": "https://openainewsdkteststorage.blob.core.windows.net/fileupload-openai-readmes-index/azure-go.md",
                    "filepath": "azure-go.md",
                    "chunk_id": "0"
                  }
                ],
                "intent": "[\"PR complete meaning\", \"definition of PR complete\", \"what is PR complete\"]"
              }
            }
          }
        ],
        "usage": {
          "prompt_tokens": 5658,
          "completion_tokens": 37,
          "total_tokens": 5695
        },
        "system_fingerprint": "fp_ee1d74bde0"
      }
    }
  ],
  "Variables": {
    "RandomSeed": "355876678"
  }
}
