{
  "Entries": [
    {
      "RequestUri": "https://Sanitized.openai.azure.com/openai/evals?api-version=2025-04-01-preview",
      "RequestMethod": "POST",
      "RequestHeaders": {
        "Accept": "application/json",
        "Accept-Encoding": "gzip, deflate",
        "api-key": "Sanitized",
        "Connection": "keep-alive",
        "Content-Length": "881",
        "Content-Type": "application/json",
        "User-Agent": "AzureOpenAI/Python 1.72.0",
        "X-Stainless-Arch": "x64",
        "X-Stainless-Async": "async:asyncio",
        "X-Stainless-Lang": "python",
        "X-Stainless-OS": "Other:XXX",
        "X-Stainless-Package-Version": "vX.X.X",
        "X-Stainless-Runtime": "CPython",
        "X-Stainless-Runtime-Version": "3.X.X"
      },
      "RequestBody": {
        "data_source_config": {
          "type": "custom",
          "item_schema": {
            "type": "object",
            "properties": {
              "query": {
                "type": "string"
              },
              "context": {
                "type": "string"
              },
              "response": {
                "type": "string"
              },
              "ground_truth": {
                "type": "string"
              }
            },
            "required": [
              "query",
              "context",
              "response",
              "ground_truth"
            ]
          }
        },
        "testing_criteria": [
          {
            "evaluation_metric": "fuzzy_match",
            "input": "{{item.query}}",
            "pass_threshold": 1.0,
            "reference": "{{item.query}}",
            "type": "text_similarity",
            "name": "Sanitized"
          },
          {
            "input": "{{item.query}}",
            "name": "Sanitized",
            "operation": "like",
            "reference": "What is",
            "type": "string_check"
          },
          {
            "input": [
              {
                "content": "{{item.query}}",
                "role": "user"
              }
            ],
            "labels": [
              "too short",
              "just right",
              "too long"
            ],
            "model": "gpt-4o",
            "name": "Sanitized",
            "passing_labels": [
              "just right"
            ],
            "type": "label_model"
          },
          {
            "input": "{{item.query}}",
            "name": "Sanitized",
            "operation": "like",
            "reference": "hello",
            "type": "string_check"
          }
        ],
        "metadata": {
          "is_foundry_eval": "true"
        }
      },
      "StatusCode": 201,
      "ResponseHeaders": {
        "apim-request-id": "63ec8413-8125-4c58-8814-68bdbda4c4c9",
        "azureml-served-by-cluster": "hyena-stage-eastus2-01",
        "Content-Encoding": "gzip",
        "Content-Type": "application/json",
        "Date": "Fri, 25 Apr 2025 16:16:47 GMT",
        "openai-organization": "Azure OpenAI",
        "openai-processing-ms": "146",
        "openai-version": "2020-10-01",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
        "Transfer-Encoding": "chunked",
        "Vary": "Accept-Encoding",
        "X-Content-Type-Options": "nosniff",
        "x-ms-region": "West US 2",
        "X-Request-ID": "1a72a513-5737-439e-90f5-120752f4e363"
      },
      "ResponseBody": {
        "object": "eval",
        "id": "Sanitized",
        "data_source_config": {
          "type": "custom",
          "schema": {
            "type": "object",
            "properties": {
              "item": {
                "type": "object",
                "properties": {
                  "query": {
                    "type": "string"
                  },
                  "context": {
                    "type": "string"
                  },
                  "response": {
                    "type": "string"
                  },
                  "ground_truth": {
                    "type": "string"
                  }
                },
                "required": [
                  "query",
                  "context",
                  "response",
                  "ground_truth"
                ]
              }
            },
            "required": [
              "item"
            ]
          }
        },
        "testing_criteria": [
          {
            "name": "Sanitized",
            "id": "Sanitized",
            "type": "text_similarity",
            "input": "{{item.query}}",
            "reference": "{{item.query}}",
            "pass_threshold": 1.0,
            "evaluation_metric": "fuzzy_match"
          },
          {
            "name": "Sanitized",
            "id": "Sanitized",
            "type": "string_check",
            "input": "{{item.query}}",
            "reference": "What is",
            "operation": "like"
          },
          {
            "name": "Sanitized",
            "id": "Sanitized",
            "type": "label_model",
            "model": "gpt-4o",
            "input": [
              {
                "type": "message",
                "role": "user",
                "content": {
                  "type": "input_text",
                  "text": "{{item.query}}"
                }
              }
            ],
            "passing_labels": [
              "just right"
            ],
            "labels": [
              "too short",
              "just right",
              "too long"
            ],
            "sampling_params": null
          },
          {
            "name": "Sanitized",
            "id": "Sanitized",
            "type": "string_check",
            "input": "{{item.query}}",
            "reference": "hello",
            "operation": "like"
          }
        ],
        "name": "Sanitized",
        "created_at": 1745597807,
        "metadata": {
          "is_foundry_eval": "true"
        }
      }
    },
    {
      "RequestUri": "https://Sanitized.openai.azure.com/openai/evals/eval_680bb56f3a588190972fb74554d39dfa/runs?api-version=2025-04-01-preview",
      "RequestMethod": "POST",
      "RequestHeaders": {
        "Accept": "application/json",
        "Accept-Encoding": "gzip, deflate",
        "api-key": "Sanitized",
        "Connection": "keep-alive",
        "Content-Length": "3320",
        "Content-Type": "application/json",
        "User-Agent": "AzureOpenAI/Python 1.72.0",
        "X-Stainless-Arch": "x64",
        "X-Stainless-Async": "async:asyncio",
        "X-Stainless-Lang": "python",
        "X-Stainless-OS": "Other:XXX",
        "X-Stainless-Package-Version": "vX.X.X",
        "X-Stainless-Runtime": "CPython",
        "X-Stainless-Runtime-Version": "3.X.X"
      },
      "RequestBody": {
        "data_source": {
          "type": "jsonl",
          "source": {
            "type": "file_content",
            "content": [
              {
                "item": {
                  "query": "How do you create a run?",
                  "context": "AML API only",
                  "response": "To create a run using the Azure Machine Learning API, you first need to create an Experiment. Once you have an experiment, you can create a Run object that is associated with that experiment. Here is some Python code that demonstrates this process:\n\n```\nfrom azureml.core import Experiment, Run\nfrom azureml.core.workspace import Workspace\n\n# Define workspace and experiment\nws = Workspace.from_config()\nexp = Experiment(workspace=ws, name='my_experiment')\n\n# Create a new run\nrun = exp.start_logging()\n```\n\nIn this code, the `from_config()` method reads the configuration file that you created when you set up your Azure Machine Learning workspace. The `Experiment` constructor creates an Experiment object that is associated with your workspace, and the `start_logging()` method creates a new Run object that is associated with the Experiment. Now you can use the `run` object to log metrics, upload files, and track other information related to your machine learning experiment.",
                  "ground_truth": "Paris is the capital of France."
                }
              },
              {
                "item": {
                  "query": "How do you log a model?",
                  "context": "Logging can be done using any OSS Sdk",
                  "response": "There are a few ways to log models in Azure Machine Learning. \n\nOne way is to use the `register_model()` method of the `Run` object. The `register_model()` method logs a model file in the Azure Machine Learning service workspace and makes it available for deployment. Here's an example:\n\n```python\nfrom azureml.core import Model\n\nmodel_path = './outputs/my_model.pkl'\nmodel = Model.register(workspace=ws, model_path=model_path, model_name='my_model')\n```\n\nThis code registers the model file located at `model_path` to the Azure Machine Learning service workspace with the name `my_model`. \n\nAnother way to log a model is to save it as an output of a `Run`. If your model generation code is part of a script or Jupyter notebook that runs as an Azure Machine Learning experiment, you can save the model file as an output of the `Run` object. Here's an example:\n\n```python\nfrom sklearn.linear_model import LogisticRegression\nfrom azureml.core.run import Run\n\n# Initialize a run object\nrun = Run.get_context()\n\n# Train your model\nX_train, y_train = ...\nclf = LogisticRegression().fit(X_train, y_train)\n\n# Save the model to the Run object's outputs directory\nmodel_path = 'outputs/model.pkl'\njoblib.dump(value=clf, filename=model_path)\n\n# Log the model as a run artifact\nrun.upload_file(name=model_path, path_or_stream=model_path)\n```\n\nIn this code, `Run.get_context()` retrieves the current run context object, which you can use to track metadata and metrics for the run. After training your model, you can use `joblib.dump()` to save the model to a file, and then log the file as an artifact of the run using `run.upload_file()`.",
                  "ground_truth": "Paris is the capital of France."
                }
              },
              {
                "item": {
                  "query": "What is the capital of France?`''\"</>{}{{]",
                  "context": "France is in Europe",
                  "response": "Paris is the capital of France.",
                  "ground_truth": "Paris is the capital of France."
                }
              }
            ]
          }
        },
        "metadata": {
          "sample_generation": "off",
          "file_format": "jsonl",
          "is_foundry_eval": "true"
        },
        "name": "Sanitized"
      },
      "StatusCode": 201,
      "ResponseHeaders": {
        "apim-request-id": "cd96c568-4fff-45ea-b1dc-b3058baaebbc",
        "azureml-served-by-cluster": "hyena-stage-eastus2-01",
        "Content-Encoding": "gzip",
        "Content-Type": "application/json",
        "Date": "Fri, 25 Apr 2025 16:16:48 GMT",
        "openai-organization": "Azure OpenAI",
        "openai-processing-ms": "932",
        "openai-version": "2020-10-01",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
        "Transfer-Encoding": "chunked",
        "Vary": "Accept-Encoding",
        "X-Content-Type-Options": "nosniff",
        "x-ms-region": "West US 2",
        "X-Request-ID": "55a459b0-4583-4d2c-b4c1-b9e10497a15f"
      },
      "ResponseBody": {
        "object": "eval.run",
        "id": "Sanitized",
        "eval_id": "eval_680bb56f3a588190972fb74554d39dfa",
        "report_url": "https://ai.azure.com/resource/evaluation",
        "status": "queued",
        "model": null,
        "name": "Sanitized",
        "created_at": 1745597808,
        "result_counts": {
          "total": 0,
          "errored": 0,
          "failed": 0,
          "passed": 0
        },
        "per_model_usage": null,
        "per_testing_criteria_results": null,
        "data_source": {
          "type": "jsonl",
          "source": {
            "type": "file_content",
            "content": [
              {
                "item": {
                  "query": "How do you create a run?",
                  "context": "AML API only",
                  "response": "To create a run using the Azure Machine Learning API, you first need to create an Experiment. Once you have an experiment, you can create a Run object that is associated with that experiment. Here is some Python code that demonstrates this process:\n\n```\nfrom azureml.core import Experiment, Run\nfrom azureml.core.workspace import Workspace\n\n# Define workspace and experiment\nws = Workspace.from_config()\nexp = Experiment(workspace=ws, name='my_experiment')\n\n# Create a new run\nrun = exp.start_logging()\n```\n\nIn this code, the `from_config()` method reads the configuration file that you created when you set up your Azure Machine Learning workspace. The `Experiment` constructor creates an Experiment object that is associated with your workspace, and the `start_logging()` method creates a new Run object that is associated with the Experiment. Now you can use the `run` object to log metrics, upload files, and track other information related to your machine learning experiment.",
                  "ground_truth": "Paris is the capital of France."
                }
              },
              {
                "item": {
                  "query": "How do you log a model?",
                  "context": "Logging can be done using any OSS Sdk",
                  "response": "There are a few ways to log models in Azure Machine Learning. \n\nOne way is to use the `register_model()` method of the `Run` object. The `register_model()` method logs a model file in the Azure Machine Learning service workspace and makes it available for deployment. Here's an example:\n\n```python\nfrom azureml.core import Model\n\nmodel_path = './outputs/my_model.pkl'\nmodel = Model.register(workspace=ws, model_path=model_path, model_name='my_model')\n```\n\nThis code registers the model file located at `model_path` to the Azure Machine Learning service workspace with the name `my_model`. \n\nAnother way to log a model is to save it as an output of a `Run`. If your model generation code is part of a script or Jupyter notebook that runs as an Azure Machine Learning experiment, you can save the model file as an output of the `Run` object. Here's an example:\n\n```python\nfrom sklearn.linear_model import LogisticRegression\nfrom azureml.core.run import Run\n\n# Initialize a run object\nrun = Run.get_context()\n\n# Train your model\nX_train, y_train = ...\nclf = LogisticRegression().fit(X_train, y_train)\n\n# Save the model to the Run object's outputs directory\nmodel_path = 'outputs/model.pkl'\njoblib.dump(value=clf, filename=model_path)\n\n# Log the model as a run artifact\nrun.upload_file(name=model_path, path_or_stream=model_path)\n```\n\nIn this code, `Run.get_context()` retrieves the current run context object, which you can use to track metadata and metrics for the run. After training your model, you can use `joblib.dump()` to save the model to a file, and then log the file as an artifact of the run using `run.upload_file()`.",
                  "ground_truth": "Paris is the capital of France."
                }
              },
              {
                "item": {
                  "query": "What is the capital of France?`''\"</>{}{{]",
                  "context": "France is in Europe",
                  "response": "Paris is the capital of France.",
                  "ground_truth": "Paris is the capital of France."
                }
              }
            ]
          }
        },
        "error": null,
        "metadata": {
          "sample_generation": "off",
          "file_format": "jsonl",
          "is_foundry_eval": "true"
        }
      }
    },
    {
      "RequestUri": "https://Sanitized.openai.azure.com/openai/evals/eval_680bb56f3a588190972fb74554d39dfa/runs/evalrun_680bb5700b7c8190b7bf2996fcddbcf6?api-version=2025-04-01-preview",
      "RequestMethod": "GET",
      "RequestHeaders": {
        "Accept": "application/json",
        "Accept-Encoding": "gzip, deflate",
        "api-key": "Sanitized",
        "Connection": "keep-alive",
        "Content-Type": "application/json",
        "User-Agent": "AzureOpenAI/Python 1.72.0",
        "X-Stainless-Arch": "x64",
        "X-Stainless-Async": "async:asyncio",
        "X-Stainless-Lang": "python",
        "X-Stainless-OS": "Other:XXX",
        "X-Stainless-Package-Version": "vX.X.X",
        "X-Stainless-Runtime": "CPython",
        "X-Stainless-Runtime-Version": "3.X.X"
      },
      "RequestBody": null,
      "StatusCode": 200,
      "ResponseHeaders": {
        "apim-request-id": "ba2a7fca-cf1d-4a2b-9250-fb06ca32f84c",
        "azureml-served-by-cluster": "hyena-stage-eastus2-01",
        "Content-Encoding": "gzip",
        "Content-Type": "application/json",
        "Date": "Fri, 25 Apr 2025 16:16:54 GMT",
        "openai-organization": "Azure OpenAI",
        "openai-processing-ms": "47",
        "openai-version": "2020-10-01",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
        "Transfer-Encoding": "chunked",
        "Vary": "Accept-Encoding",
        "X-Content-Type-Options": "nosniff",
        "x-ms-region": "West US 2",
        "X-Request-ID": "181baf12-3f3e-4e95-8a60-0a720dc86ef5"
      },
      "ResponseBody": {
        "object": "eval.run",
        "id": "Sanitized",
        "eval_id": "eval_680bb56f3a588190972fb74554d39dfa",
        "report_url": "https://platform.openai.com/evaluations/eval_680bb56f3a588190972fb74554d39dfa?project_id=703bc37069544191af3e583350c7ec61&run_id=evalrun_680bb5700b7c8190b7bf2996fcddbcf6",
        "status": "in_progress",
        "model": null,
        "name": "Sanitized",
        "created_at": 1745597808,
        "result_counts": {
          "total": 0,
          "errored": 0,
          "failed": 0,
          "passed": 0
        },
        "per_model_usage": null,
        "per_testing_criteria_results": null,
        "data_source": {
          "type": "jsonl",
          "source": {
            "type": "file_content",
            "content": [
              {
                "item": {
                  "query": "How do you create a run?",
                  "context": "AML API only",
                  "response": "To create a run using the Azure Machine Learning API, you first need to create an Experiment. Once you have an experiment, you can create a Run object that is associated with that experiment. Here is some Python code that demonstrates this process:\n\n```\nfrom azureml.core import Experiment, Run\nfrom azureml.core.workspace import Workspace\n\n# Define workspace and experiment\nws = Workspace.from_config()\nexp = Experiment(workspace=ws, name='my_experiment')\n\n# Create a new run\nrun = exp.start_logging()\n```\n\nIn this code, the `from_config()` method reads the configuration file that you created when you set up your Azure Machine Learning workspace. The `Experiment` constructor creates an Experiment object that is associated with your workspace, and the `start_logging()` method creates a new Run object that is associated with the Experiment. Now you can use the `run` object to log metrics, upload files, and track other information related to your machine learning experiment.",
                  "ground_truth": "Paris is the capital of France."
                }
              },
              {
                "item": {
                  "query": "How do you log a model?",
                  "context": "Logging can be done using any OSS Sdk",
                  "response": "There are a few ways to log models in Azure Machine Learning. \n\nOne way is to use the `register_model()` method of the `Run` object. The `register_model()` method logs a model file in the Azure Machine Learning service workspace and makes it available for deployment. Here's an example:\n\n```python\nfrom azureml.core import Model\n\nmodel_path = './outputs/my_model.pkl'\nmodel = Model.register(workspace=ws, model_path=model_path, model_name='my_model')\n```\n\nThis code registers the model file located at `model_path` to the Azure Machine Learning service workspace with the name `my_model`. \n\nAnother way to log a model is to save it as an output of a `Run`. If your model generation code is part of a script or Jupyter notebook that runs as an Azure Machine Learning experiment, you can save the model file as an output of the `Run` object. Here's an example:\n\n```python\nfrom sklearn.linear_model import LogisticRegression\nfrom azureml.core.run import Run\n\n# Initialize a run object\nrun = Run.get_context()\n\n# Train your model\nX_train, y_train = ...\nclf = LogisticRegression().fit(X_train, y_train)\n\n# Save the model to the Run object's outputs directory\nmodel_path = 'outputs/model.pkl'\njoblib.dump(value=clf, filename=model_path)\n\n# Log the model as a run artifact\nrun.upload_file(name=model_path, path_or_stream=model_path)\n```\n\nIn this code, `Run.get_context()` retrieves the current run context object, which you can use to track metadata and metrics for the run. After training your model, you can use `joblib.dump()` to save the model to a file, and then log the file as an artifact of the run using `run.upload_file()`.",
                  "ground_truth": "Paris is the capital of France."
                }
              },
              {
                "item": {
                  "query": "What is the capital of France?`''\"</>{}{{]",
                  "context": "France is in Europe",
                  "response": "Paris is the capital of France.",
                  "ground_truth": "Paris is the capital of France."
                }
              }
            ]
          }
        },
        "error": null,
        "metadata": {
          "sample_generation": "off",
          "file_format": "jsonl",
          "is_foundry_eval": "true"
        }
      }
    },
    {
      "RequestUri": "https://Sanitized.openai.azure.com/openai/evals/eval_680bb56f3a588190972fb74554d39dfa/runs/evalrun_680bb5700b7c8190b7bf2996fcddbcf6?api-version=2025-04-01-preview",
      "RequestMethod": "GET",
      "RequestHeaders": {
        "Accept": "application/json",
        "Accept-Encoding": "gzip, deflate",
        "api-key": "Sanitized",
        "Connection": "keep-alive",
        "Content-Type": "application/json",
        "User-Agent": "AzureOpenAI/Python 1.72.0",
        "X-Stainless-Arch": "x64",
        "X-Stainless-Async": "async:asyncio",
        "X-Stainless-Lang": "python",
        "X-Stainless-OS": "Other:XXX",
        "X-Stainless-Package-Version": "vX.X.X",
        "X-Stainless-Runtime": "CPython",
        "X-Stainless-Runtime-Version": "3.X.X"
      },
      "RequestBody": null,
      "StatusCode": 200,
      "ResponseHeaders": {
        "apim-request-id": "a2495bae-5a72-4722-91bd-40388acd7bc3",
        "azureml-served-by-cluster": "hyena-stage-eastus2-01",
        "Content-Encoding": "gzip",
        "Content-Type": "application/json",
        "Date": "Fri, 25 Apr 2025 16:17:00 GMT",
        "openai-organization": "Azure OpenAI",
        "openai-processing-ms": "27",
        "openai-version": "2020-10-01",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
        "Transfer-Encoding": "chunked",
        "Vary": "Accept-Encoding",
        "X-Content-Type-Options": "nosniff",
        "x-ms-region": "West US 2",
        "X-Request-ID": "7cb7fb89-3b49-4d13-83aa-dc6f7b71f967"
      },
      "ResponseBody": {
        "object": "eval.run",
        "id": "Sanitized",
        "eval_id": "eval_680bb56f3a588190972fb74554d39dfa",
        "report_url": "https://platform.openai.com/evaluations/eval_680bb56f3a588190972fb74554d39dfa?project_id=703bc37069544191af3e583350c7ec61&run_id=evalrun_680bb5700b7c8190b7bf2996fcddbcf6",
        "status": "completed",
        "model": null,
        "name": "Sanitized",
        "created_at": 1745597808,
        "result_counts": {
          "total": 3,
          "errored": 0,
          "failed": 3,
          "passed": 0
        },
        "per_model_usage": [
          {
            "model_name": "gpt-4o-2024-11-20",
            "invocation_count": 3,
            "prompt_tokens": 397,
            "completion_tokens": 253,
            "total_tokens": 650,
            "cached_tokens": 0
          }
        ],
        "per_testing_criteria_results": [
          {
            "testing_criteria": "similarity-8400cafa-36fa-4173-a9e9-157f2ebd8e68",
            "passed": 3,
            "failed": 0
          },
          {
            "testing_criteria": "starts with what is-c63ac1eb-2f7f-4ee1-a238-d086a2c32adc",
            "passed": 1,
            "failed": 2
          },
          {
            "testing_criteria": "label-db29d7f0-d150-4fee-98c6-750d0791b3c5",
            "passed": 3,
            "failed": 0
          },
          {
            "testing_criteria": "contains hello-5a4d62f2-c194-4188-8296-d19b87f46a1a",
            "passed": 0,
            "failed": 3
          }
        ],
        "data_source": {
          "type": "jsonl",
          "source": {
            "type": "file_content",
            "content": [
              {
                "item": {
                  "query": "How do you create a run?",
                  "context": "AML API only",
                  "response": "To create a run using the Azure Machine Learning API, you first need to create an Experiment. Once you have an experiment, you can create a Run object that is associated with that experiment. Here is some Python code that demonstrates this process:\n\n```\nfrom azureml.core import Experiment, Run\nfrom azureml.core.workspace import Workspace\n\n# Define workspace and experiment\nws = Workspace.from_config()\nexp = Experiment(workspace=ws, name='my_experiment')\n\n# Create a new run\nrun = exp.start_logging()\n```\n\nIn this code, the `from_config()` method reads the configuration file that you created when you set up your Azure Machine Learning workspace. The `Experiment` constructor creates an Experiment object that is associated with your workspace, and the `start_logging()` method creates a new Run object that is associated with the Experiment. Now you can use the `run` object to log metrics, upload files, and track other information related to your machine learning experiment.",
                  "ground_truth": "Paris is the capital of France."
                }
              },
              {
                "item": {
                  "query": "How do you log a model?",
                  "context": "Logging can be done using any OSS Sdk",
                  "response": "There are a few ways to log models in Azure Machine Learning. \n\nOne way is to use the `register_model()` method of the `Run` object. The `register_model()` method logs a model file in the Azure Machine Learning service workspace and makes it available for deployment. Here's an example:\n\n```python\nfrom azureml.core import Model\n\nmodel_path = './outputs/my_model.pkl'\nmodel = Model.register(workspace=ws, model_path=model_path, model_name='my_model')\n```\n\nThis code registers the model file located at `model_path` to the Azure Machine Learning service workspace with the name `my_model`. \n\nAnother way to log a model is to save it as an output of a `Run`. If your model generation code is part of a script or Jupyter notebook that runs as an Azure Machine Learning experiment, you can save the model file as an output of the `Run` object. Here's an example:\n\n```python\nfrom sklearn.linear_model import LogisticRegression\nfrom azureml.core.run import Run\n\n# Initialize a run object\nrun = Run.get_context()\n\n# Train your model\nX_train, y_train = ...\nclf = LogisticRegression().fit(X_train, y_train)\n\n# Save the model to the Run object's outputs directory\nmodel_path = 'outputs/model.pkl'\njoblib.dump(value=clf, filename=model_path)\n\n# Log the model as a run artifact\nrun.upload_file(name=model_path, path_or_stream=model_path)\n```\n\nIn this code, `Run.get_context()` retrieves the current run context object, which you can use to track metadata and metrics for the run. After training your model, you can use `joblib.dump()` to save the model to a file, and then log the file as an artifact of the run using `run.upload_file()`.",
                  "ground_truth": "Paris is the capital of France."
                }
              },
              {
                "item": {
                  "query": "What is the capital of France?`''\"</>{}{{]",
                  "context": "France is in Europe",
                  "response": "Paris is the capital of France.",
                  "ground_truth": "Paris is the capital of France."
                }
              }
            ]
          }
        },
        "error": null,
        "metadata": {
          "sample_generation": "off",
          "file_format": "jsonl",
          "is_foundry_eval": "true"
        }
      }
    },
    {
      "RequestUri": "https://Sanitized.openai.azure.com/openai/evals/eval_680bb56f3a588190972fb74554d39dfa/runs/evalrun_680bb5700b7c8190b7bf2996fcddbcf6/output_items?api-version=2025-04-01-preview",
      "RequestMethod": "GET",
      "RequestHeaders": {
        "Accept": "application/json",
        "Accept-Encoding": "gzip, deflate",
        "api-key": "Sanitized",
        "Connection": "keep-alive",
        "Content-Type": "application/json",
        "User-Agent": "AzureOpenAI/Python 1.72.0",
        "X-Stainless-Arch": "x64",
        "X-Stainless-Async": "async:asyncio",
        "X-Stainless-Lang": "python",
        "X-Stainless-OS": "Other:XXX",
        "X-Stainless-Package-Version": "vX.X.X",
        "X-Stainless-Runtime": "CPython",
        "X-Stainless-Runtime-Version": "3.X.X"
      },
      "RequestBody": null,
      "StatusCode": 200,
      "ResponseHeaders": {
        "apim-request-id": "c3114675-fb1d-46c3-9fc6-b96c802385a1",
        "azureml-served-by-cluster": "hyena-stage-eastus2-01",
        "Content-Encoding": "gzip",
        "Content-Type": "application/json",
        "Date": "Fri, 25 Apr 2025 16:17:00 GMT",
        "openai-organization": "Azure OpenAI",
        "openai-processing-ms": "57",
        "openai-version": "2020-10-01",
        "Strict-Transport-Security": "max-age=31536000; includeSubDomains; preload",
        "Transfer-Encoding": "chunked",
        "Vary": "Accept-Encoding",
        "X-Content-Type-Options": "nosniff",
        "x-ms-region": "West US 2",
        "X-Request-ID": "2a49f49d-27eb-463e-89ae-1332c44d2d11"
      },
      "ResponseBody": {
        "object": "list",
        "data": [
          {
            "object": "eval.run.output_item",
            "id": "Sanitized",
            "created_at": 1745597818,
            "run_id": "evalrun_680bb5700b7c8190b7bf2996fcddbcf6",
            "eval_id": "eval_680bb56f3a588190972fb74554d39dfa",
            "status": "fail",
            "_datasource_item_content_hash": "5a41dda8f9afa7a7f46deb5040a11aacb2cb360fbfbce1c550b075e07898e5c5",
            "datasource_item_id": 2,
            "datasource_item": {
              "query": "What is the capital of France?`''\"</>{}{{]",
              "context": "France is in Europe",
              "response": "Paris is the capital of France.",
              "ground_truth": "Paris is the capital of France."
            },
            "results": [
              {
                "name": "Sanitized",
                "sample": null,
                "passed": true,
                "score": 1.0
              },
              {
                "name": "Sanitized",
                "sample": null,
                "passed": true,
                "score": 1.0
              },
              {
                "name": "Sanitized",
                "sample": {
                  "input": [
                    {
                      "role": "user",
                      "content": "What is the capital of France?`''\"</>{}{{]",
                      "tool_call_id": null,
                      "tool_calls": null,
                      "function_call": null
                    }
                  ],
                  "output": [
                    {
                      "role": "assistant",
                      "content": "{\"steps\":[{\"description\":\"The query asks for the capital of France.\",\"conclusion\":\"Paris is the capital of France.\"}],\"result\":\"just right\"}",
                      "tool_call_id": null,
                      "tool_calls": null,
                      "function_call": null
                    }
                  ],
                  "finish_reason": "stop",
                  "model": "gpt-4o-2024-11-20",
                  "usage": {
                    "total_tokens": 170,
                    "completion_tokens": 33,
                    "prompt_tokens": 137,
                    "cached_tokens": 0
                  },
                  "error": null,
                  "temperature": 1.0,
                  "max_completion_tokens": 4096,
                  "top_p": 1.0,
                  "seed": null
                },
                "passed": true,
                "score": 1.0
              },
              {
                "name": "Sanitized",
                "sample": null,
                "passed": false,
                "score": 0.0
              }
            ]
          },
          {
            "object": "eval.run.output_item",
            "id": "Sanitized",
            "created_at": 1745597818,
            "run_id": "evalrun_680bb5700b7c8190b7bf2996fcddbcf6",
            "eval_id": "eval_680bb56f3a588190972fb74554d39dfa",
            "status": "fail",
            "_datasource_item_content_hash": "c460e6ebb57e37d316f208f8d381859312ea1da1926fa7278c0a814fc7254ca9",
            "datasource_item_id": 0,
            "datasource_item": {
              "query": "How do you create a run?",
              "context": "AML API only",
              "response": "To create a run using the Azure Machine Learning API, you first need to create an Experiment. Once you have an experiment, you can create a Run object that is associated with that experiment. Here is some Python code that demonstrates this process:\n\n```\nfrom azureml.core import Experiment, Run\nfrom azureml.core.workspace import Workspace\n\n# Define workspace and experiment\nws = Workspace.from_config()\nexp = Experiment(workspace=ws, name='my_experiment')\n\n# Create a new run\nrun = exp.start_logging()\n```\n\nIn this code, the `from_config()` method reads the configuration file that you created when you set up your Azure Machine Learning workspace. The `Experiment` constructor creates an Experiment object that is associated with your workspace, and the `start_logging()` method creates a new Run object that is associated with the Experiment. Now you can use the `run` object to log metrics, upload files, and track other information related to your machine learning experiment.",
              "ground_truth": "Paris is the capital of France."
            },
            "results": [
              {
                "name": "Sanitized",
                "sample": null,
                "passed": true,
                "score": 1.0
              },
              {
                "name": "Sanitized",
                "sample": null,
                "passed": false,
                "score": 0.0
              },
              {
                "name": "Sanitized",
                "sample": {
                  "input": [
                    {
                      "role": "user",
                      "content": "How do you create a run?",
                      "tool_call_id": null,
                      "tool_calls": null,
                      "function_call": null
                    }
                  ],
                  "output": [
                    {
                      "role": "assistant",
                      "content": "{\"steps\":[{\"description\":\"To determine how to create a 'run', we need to analyze the context and understand what 'run' refers to.\",\"conclusion\":\"Analyze and clarify the context for the term 'run'.\"},{\"description\":\"A 'run' can have multiple meanings, such as a software execution, a sporting activity, a production process, or a sequence of events.\",\"conclusion\":\"Identify the specific context for 'run'.\"},{\"description\":\"The steps to create a 'run' will depend on the identified context and typically involve gathering prerequisites and executing the defined process.\",\"conclusion\":\"The creation process is context-dependent and involves preparation and initiation.\"}],\"result\":\"just right\"}",
                      "tool_call_id": null,
                      "tool_calls": null,
                      "function_call": null
                    }
                  ],
                  "finish_reason": "stop",
                  "model": "gpt-4o-2024-11-20",
                  "usage": {
                    "total_tokens": 270,
                    "completion_tokens": 140,
                    "prompt_tokens": 130,
                    "cached_tokens": 0
                  },
                  "error": null,
                  "temperature": 1.0,
                  "max_completion_tokens": 4096,
                  "top_p": 1.0,
                  "seed": null
                },
                "passed": true,
                "score": 1.0
              },
              {
                "name": "Sanitized",
                "sample": null,
                "passed": false,
                "score": 0.0
              }
            ]
          },
          {
            "object": "eval.run.output_item",
            "id": "Sanitized",
            "created_at": 1745597817,
            "run_id": "evalrun_680bb5700b7c8190b7bf2996fcddbcf6",
            "eval_id": "eval_680bb56f3a588190972fb74554d39dfa",
            "status": "fail",
            "_datasource_item_content_hash": "3920f787a5bbab5152368e46d6202ff79cd9ed5e53532e1d541d5991d8a79c0f",
            "datasource_item_id": 1,
            "datasource_item": {
              "query": "How do you log a model?",
              "context": "Logging can be done using any OSS Sdk",
              "response": "There are a few ways to log models in Azure Machine Learning. \n\nOne way is to use the `register_model()` method of the `Run` object. The `register_model()` method logs a model file in the Azure Machine Learning service workspace and makes it available for deployment. Here's an example:\n\n```python\nfrom azureml.core import Model\n\nmodel_path = './outputs/my_model.pkl'\nmodel = Model.register(workspace=ws, model_path=model_path, model_name='my_model')\n```\n\nThis code registers the model file located at `model_path` to the Azure Machine Learning service workspace with the name `my_model`. \n\nAnother way to log a model is to save it as an output of a `Run`. If your model generation code is part of a script or Jupyter notebook that runs as an Azure Machine Learning experiment, you can save the model file as an output of the `Run` object. Here's an example:\n\n```python\nfrom sklearn.linear_model import LogisticRegression\nfrom azureml.core.run import Run\n\n# Initialize a run object\nrun = Run.get_context()\n\n# Train your model\nX_train, y_train = ...\nclf = LogisticRegression().fit(X_train, y_train)\n\n# Save the model to the Run object's outputs directory\nmodel_path = 'outputs/model.pkl'\njoblib.dump(value=clf, filename=model_path)\n\n# Log the model as a run artifact\nrun.upload_file(name=model_path, path_or_stream=model_path)\n```\n\nIn this code, `Run.get_context()` retrieves the current run context object, which you can use to track metadata and metrics for the run. After training your model, you can use `joblib.dump()` to save the model to a file, and then log the file as an artifact of the run using `run.upload_file()`.",
              "ground_truth": "Paris is the capital of France."
            },
            "results": [
              {
                "name": "Sanitized",
                "sample": null,
                "passed": true,
                "score": 1.0
              },
              {
                "name": "Sanitized",
                "sample": null,
                "passed": false,
                "score": 0.0
              },
              {
                "name": "Sanitized",
                "sample": {
                  "input": [
                    {
                      "role": "user",
                      "content": "How do you log a model?",
                      "tool_call_id": null,
                      "tool_calls": null,
                      "function_call": null
                    }
                  ],
                  "output": [
                    {
                      "role": "assistant",
                      "content": "{\"steps\":[{\"description\":\"Consider how detailed the user's question about logging a model is.\",\"conclusion\":\"The question is sufficiently general and detailed to warrant a comprehensive response.\"},{\"description\":\"Reflect on the necessary completeness of the response by thinking about steps to explain the concept.\",\"conclusion\":\"The length of the response should align with the user's expectation and clarity requirements.\"}],\"result\":\"just right\"}",
                      "tool_call_id": null,
                      "tool_calls": null,
                      "function_call": null
                    }
                  ],
                  "finish_reason": "stop",
                  "model": "gpt-4o-2024-11-20",
                  "usage": {
                    "total_tokens": 210,
                    "completion_tokens": 80,
                    "prompt_tokens": 130,
                    "cached_tokens": 0
                  },
                  "error": null,
                  "temperature": 1.0,
                  "max_completion_tokens": 4096,
                  "top_p": 1.0,
                  "seed": null
                },
                "passed": true,
                "score": 1.0
              },
              {
                "name": "Sanitized",
                "sample": null,
                "passed": false,
                "score": 0.0
              }
            ]
          }
        ],
        "first_id": "outputitem_680bb57a53208190bfa4dc2c10866247",
        "last_id": "outputitem_680bb579dc588190aa3d983cffcc90b0",
        "has_more": false
      }
    }
  ],
  "Variables": {}
}
